{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8177701,
     "sourceType": "datasetVersion",
     "datasetId": 4840814
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install timm\n",
    "!pip install peft\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import timm\n",
    "import imageio.v2 as imageio\n",
    "from torchvision.transforms import v2\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-23T18:13:23.313456Z",
     "iopub.execute_input": "2024-04-23T18:13:23.314286Z",
     "iopub.status.idle": "2024-04-23T18:13:59.536443Z",
     "shell.execute_reply.started": "2024-04-23T18:13:23.314239Z",
     "shell.execute_reply": "2024-04-23T18:13:59.535315Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.22.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\nCollecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.1/199.1 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m0m\n\u001B[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.10.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:04.120384Z",
     "iopub.execute_input": "2024-04-23T18:16:04.120868Z",
     "iopub.status.idle": "2024-04-23T18:16:04.125231Z",
     "shell.execute_reply.started": "2024-04-23T18:16:04.120836Z",
     "shell.execute_reply": "2024-04-23T18:16:04.124319Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "metadata = pd.read_csv('/kaggle/input/ham1000/dataverse_files — копия/HAM10000_metadata.csv')\n",
    "metadata.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:04.547678Z",
     "iopub.execute_input": "2024-04-23T18:16:04.548101Z",
     "iopub.status.idle": "2024-04-23T18:16:04.630987Z",
     "shell.execute_reply.started": "2024-04-23T18:16:04.548000Z",
     "shell.execute_reply": "2024-04-23T18:16:04.630000Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n\n        dataset  \n0  vidir_modern  \n1  vidir_modern  \n2  vidir_modern  \n3  vidir_modern  \n4  vidir_modern  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>vidir_modern</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "first_part = glob('/kaggle/input/ham1000/dataverse_files — копия/HAM10000_images_part_1/*')\n",
    "second_part = glob('/kaggle/input/ham1000/dataverse_files — копия/HAM10000_images_part_2/*')\n",
    "first_part.extend(second_part)\n",
    "first_part_id = [x.split('/')[-1].split('.')[0] for x in first_part]\n",
    "\n",
    "df_paths = pd.DataFrame({'id': first_part_id, 'path': first_part})\n",
    "df_paths.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:04.970373Z",
     "iopub.execute_input": "2024-04-23T18:16:04.970731Z",
     "iopub.status.idle": "2024-04-23T18:16:06.142070Z",
     "shell.execute_reply.started": "2024-04-23T18:16:04.970702Z",
     "shell.execute_reply": "2024-04-23T18:16:06.141176Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "             id                                               path\n0  ISIC_0028933  /kaggle/input/ham1000/dataverse_files — копия/...\n1  ISIC_0028394  /kaggle/input/ham1000/dataverse_files — копия/...\n2  ISIC_0027799  /kaggle/input/ham1000/dataverse_files — копия/...\n3  ISIC_0028100  /kaggle/input/ham1000/dataverse_files — копия/...\n4  ISIC_0027960  /kaggle/input/ham1000/dataverse_files — копия/...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0028933</td>\n      <td>/kaggle/input/ham1000/dataverse_files — копия/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0028394</td>\n      <td>/kaggle/input/ham1000/dataverse_files — копия/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0027799</td>\n      <td>/kaggle/input/ham1000/dataverse_files — копия/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ISIC_0028100</td>\n      <td>/kaggle/input/ham1000/dataverse_files — копия/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ISIC_0027960</td>\n      <td>/kaggle/input/ham1000/dataverse_files — копия/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "metadata = metadata.merge(df_paths, left_on='image_id', right_on='id').drop(columns=['id'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:06.143870Z",
     "iopub.execute_input": "2024-04-23T18:16:06.144477Z",
     "iopub.status.idle": "2024-04-23T18:16:06.171529Z",
     "shell.execute_reply.started": "2024-04-23T18:16:06.144441Z",
     "shell.execute_reply": "2024-04-23T18:16:06.170640Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe.values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = np.array(imageio.imread(img_path, pilmode='RGB'))\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        img = self.transform(img)\n",
    "        return img, label"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:06.173220Z",
     "iopub.execute_input": "2024-04-23T18:16:06.173862Z",
     "iopub.status.idle": "2024-04-23T18:16:06.180720Z",
     "shell.execute_reply.started": "2024-04-23T18:16:06.173823Z",
     "shell.execute_reply": "2024-04-23T18:16:06.179842Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transforms_train = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(size=(518, 518)),\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "transforms_test = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(size=(518, 518)),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:07.147181Z",
     "iopub.execute_input": "2024-04-23T18:16:07.147901Z",
     "iopub.status.idle": "2024-04-23T18:16:07.154730Z",
     "shell.execute_reply.started": "2024-04-23T18:16:07.147863Z",
     "shell.execute_reply": "2024-04-23T18:16:07.153785Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "le = LabelEncoder()\n",
    "metadata['dx_encoded'] = le.fit_transform(metadata['dx'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:07.701370Z",
     "iopub.execute_input": "2024-04-23T18:16:07.702135Z",
     "iopub.status.idle": "2024-04-23T18:16:07.709863Z",
     "shell.execute_reply.started": "2024-04-23T18:16:07.702101Z",
     "shell.execute_reply": "2024-04-23T18:16:07.708871Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(metadata.shape[0]), test_size=0.3,\n",
    "                                            random_state=0)\n",
    "trainset = ClassificationDataset(metadata.iloc[train_idx][['path', 'dx_encoded']],\n",
    "                                 transform=transforms_train)\n",
    "valset = ClassificationDataset(metadata.iloc[valid_idx][['path', 'dx_encoded']],\n",
    "                               transform=transforms_test)\n",
    "\n",
    "def collate_fn(data):\n",
    "    images, labels = zip(*data)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return images.float(), labels.long()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                           shuffle=True, num_workers=2, collate_fn = collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2, collate_fn = collate_fn)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:08.054302Z",
     "iopub.execute_input": "2024-04-23T18:16:08.054861Z",
     "iopub.status.idle": "2024-04-23T18:16:08.071540Z",
     "shell.execute_reply.started": "2024-04-23T18:16:08.054832Z",
     "shell.execute_reply": "2024-04-23T18:16:08.070705Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:08.550206Z",
     "iopub.execute_input": "2024-04-23T18:16:08.550555Z",
     "iopub.status.idle": "2024-04-23T18:16:08.604863Z",
     "shell.execute_reply.started": "2024-04-23T18:16:08.550526Z",
     "shell.execute_reply": "2024-04-23T18:16:08.603930Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda:0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "num_classes = 7\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "model=model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T08:21:41.313705Z",
     "iopub.execute_input": "2024-04-21T08:21:41.318413Z",
     "iopub.status.idle": "2024-04-21T08:21:43.421784Z",
     "shell.execute_reply.started": "2024-04-21T08:21:41.318368Z",
     "shell.execute_reply": "2024-04-21T08:21:43.420602Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf5adfe8d652491488b55eca5edc425f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def test(model, loader):\n",
    "    loss_log = []\n",
    "    acc_log = []\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(data)\n",
    "            loss = nn.functional.cross_entropy(predictions, target)\n",
    "\n",
    "            loss_log.append(loss.item())\n",
    "\n",
    "            acc = sum(torch.argmax(predictions.cpu(), dim=1) ==\n",
    "                   target.cpu()) / target.cpu().shape[0]\n",
    "\n",
    "            acc_log.append(acc.item())\n",
    "\n",
    "    return np.mean(loss_log), np.mean(acc_log)\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader):\n",
    "    loss_log = []\n",
    "    acc_log = []\n",
    "    model.train()\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        loss = nn.functional.cross_entropy(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = sum(torch.argmax(predictions.cpu(), dim=1) ==\n",
    "                   target.cpu()) / target.cpu().shape[0]\n",
    "\n",
    "        acc_log.append(acc.item())\n",
    "\n",
    "    return loss_log, acc_log\n",
    "\n",
    "def train(model, optimizer, n_epochs, train_loader, val_loader, scheduler=None):\n",
    "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, optimizer, train_loader)\n",
    "        val_loss, val_acc = test(model, val_loader)\n",
    "\n",
    "        train_loss_log.extend(train_loss)\n",
    "        train_acc_log.extend(train_acc)\n",
    "\n",
    "        val_loss_log.append(val_loss)\n",
    "        val_acc_log.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n",
    "        print(f\" val loss: {val_loss}, val acc: {val_acc}\\n\")\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "    return train_loss_log, train_acc_log, val_loss_log, val_acc_log"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T19:40:41.271964Z",
     "iopub.execute_input": "2024-04-23T19:40:41.273025Z",
     "iopub.status.idle": "2024-04-23T19:40:41.287093Z",
     "shell.execute_reply.started": "2024-04-23T19:40:41.272986Z",
     "shell.execute_reply": "2024-04-23T19:40:41.286092Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, optimizer,\n",
    "                                                                 10, train_loader, val_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T20:19:19.120919Z",
     "iopub.execute_input": "2024-04-20T20:19:19.121218Z",
     "iopub.status.idle": "2024-04-20T20:36:51.543722Z",
     "shell.execute_reply.started": "2024-04-20T20:19:19.121194Z",
     "shell.execute_reply": "2024-04-20T20:36:51.542641Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 0\n train loss: 0.8001652205532247, train acc: 0.7207386363636363\n val loss: 0.6571267729109906, val acc: 0.7714026963457148\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1\n train loss: 0.5933692026544701, train acc: 0.7894886363636363\n val loss: 0.8654627473430431, val acc: 0.7222349599320838\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 2\n train loss: 0.5173066445372322, train acc: 0.8166193181818182\n val loss: 0.6269754699253022, val acc: 0.7900884994801055\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 3\n train loss: 0.4336710686033422, train acc: 0.845028409090909\n val loss: 0.5195487839110354, val acc: 0.8166154623031616\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 4\n train loss: 0.36045251884074375, train acc: 0.8663352272727273\n val loss: 0.500482254047343, val acc: 0.8332721939746369\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 5\n train loss: 0.2980596147900955, train acc: 0.8897727272727273\n val loss: 0.48118413223865186, val acc: 0.8250641968656094\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 6\n train loss: 0.2733974726870656, train acc: 0.9012784090909091\n val loss: 0.565740058238202, val acc: 0.8293172228843608\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 7\n train loss: 0.25113705433905126, train acc: 0.9102272727272728\n val loss: 0.47533258890852015, val acc: 0.8456071165013821\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 8\n train loss: 0.1957782373509624, train acc: 0.9315340909090909\n val loss: 0.5422119561662065, val acc: 0.8310482394188008\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 9\n train loss: 0.16190014078112488, train acc: 0.9420454545454545\n val loss: 0.5167277663787628, val acc: 0.8509950479294391\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ViT base"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('vit_base_patch16_224.augreg2_in21k_ft_in1k', pretrained=True)\n",
    "num_classes = 7\n",
    "\n",
    "num_features = model.head.in_features\n",
    "\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "model=model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T20:51:02.036306Z",
     "iopub.execute_input": "2024-04-20T20:51:02.037071Z",
     "iopub.status.idle": "2024-04-20T20:51:03.974966Z",
     "shell.execute_reply.started": "2024-04-20T20:51:02.037039Z",
     "shell.execute_reply": "2024-04-20T20:51:03.973659Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, optimizer,\n",
    "                                                                 10, train_loader, val_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T20:51:36.650772Z",
     "iopub.execute_input": "2024-04-20T20:51:36.651513Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 0\n train loss: 1.2499654177576303, train acc: 0.6526988636363636\n val loss: 1.0815122165578477, val acc: 0.6688715149747565\n\nEpoch 1\n train loss: 0.9677933809432117, train acc: 0.6616477272727272\n val loss: 1.1011183198462142, val acc: 0.6416452679228275\n\nEpoch 2\n train loss: 0.9465008036656813, train acc: 0.6703125\n val loss: 0.903980958651989, val acc: 0.6789480923338139\n\nEpoch 3\n train loss: 0.8703149643980644, train acc: 0.6822443181818182\n val loss: 0.8716304536829603, val acc: 0.6802090977100615\n\nEpoch 4\n train loss: 0.8335374092852528, train acc: 0.6938920454545454\n val loss: 0.8588976143522465, val acc: 0.6823069516648638\n\nEpoch 5\n train loss: 0.8618105301802809, train acc: 0.6855113636363637\n val loss: 0.9298524117850243, val acc: 0.643972395582402\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base dinov2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtimm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvit_base_patch14_dinov2.lvd142m\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117\u001B[0m, in \u001B[0;36mcreate_model\u001B[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m create_fn \u001B[38;5;241m=\u001B[39m model_entrypoint(model_name)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_layer_config(scriptable\u001B[38;5;241m=\u001B[39mscriptable, exportable\u001B[38;5;241m=\u001B[39mexportable, no_jit\u001B[38;5;241m=\u001B[39mno_jit):\n\u001B[0;32m--> 117\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_fn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_cfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_cfg_overlay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained_cfg_overlay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m checkpoint_path:\n\u001B[1;32m    125\u001B[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/vision_transformer.py:2452\u001B[0m, in \u001B[0;36mvit_base_patch14_dinov2\u001B[0;34m(pretrained, **kwargs)\u001B[0m\n\u001B[1;32m   2449\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" ViT-B/14 for DINOv2\u001B[39;00m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2451\u001B[0m model_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(patch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m14\u001B[39m, embed_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m768\u001B[39m, depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12\u001B[39m, num_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12\u001B[39m, init_values\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-5\u001B[39m, img_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m518\u001B[39m)\n\u001B[0;32m-> 2452\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43m_create_vision_transformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2453\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvit_base_patch14_dinov2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/vision_transformer.py:1781\u001B[0m, in \u001B[0;36m_create_vision_transformer\u001B[0;34m(variant, pretrained, **kwargs)\u001B[0m\n\u001B[1;32m   1778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msiglip\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m variant \u001B[38;5;129;01mand\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mglobal_pool\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmap\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m   1779\u001B[0m     strict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1781\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuild_model_with_cfg\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mVisionTransformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_filter_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_filter_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_strict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1787\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1788\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_builder.py:398\u001B[0m, in \u001B[0;36mbuild_model_with_cfg\u001B[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;66;03m# Instantiate the model\u001B[39;00m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_cfg \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 398\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    400\u001B[0m     model \u001B[38;5;241m=\u001B[39m model_cls(cfg\u001B[38;5;241m=\u001B[39mmodel_cfg, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/vision_transformer.py:540\u001B[0m, in \u001B[0;36mVisionTransformer.__init__\u001B[0;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, no_embed_class, reg_tokens, pre_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, fix_init, embed_layer, norm_layer, act_layer, block_fn, mlp_layer)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, num_classes) \u001B[38;5;28;01mif\u001B[39;00m num_classes \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m nn\u001B[38;5;241m.\u001B[39mIdentity()\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight_init \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskip\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 540\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight_init\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fix_init:\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfix_init_weight()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/vision_transformer.py:558\u001B[0m, in \u001B[0;36mVisionTransformer.init_weights\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    557\u001B[0m     nn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mnormal_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_token, std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-6\u001B[39m)\n\u001B[0;32m--> 558\u001B[0m \u001B[43mnamed_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_init_weights_vit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_bias\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_manipulate.py:34\u001B[0m, in \u001B[0;36mnamed_apply\u001B[0;34m(fn, module, name, depth_first, include_root)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child_name, child_module \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m     33\u001B[0m     child_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin((name, child_name)) \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;28;01melse\u001B[39;00m child_name\n\u001B[0;32m---> 34\u001B[0m     \u001B[43mnamed_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdepth_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depth_first \u001B[38;5;129;01mand\u001B[39;00m include_root:\n\u001B[1;32m     36\u001B[0m     fn(module\u001B[38;5;241m=\u001B[39mmodule, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_manipulate.py:34\u001B[0m, in \u001B[0;36mnamed_apply\u001B[0;34m(fn, module, name, depth_first, include_root)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child_name, child_module \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m     33\u001B[0m     child_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin((name, child_name)) \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;28;01melse\u001B[39;00m child_name\n\u001B[0;32m---> 34\u001B[0m     \u001B[43mnamed_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdepth_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depth_first \u001B[38;5;129;01mand\u001B[39;00m include_root:\n\u001B[1;32m     36\u001B[0m     fn(module\u001B[38;5;241m=\u001B[39mmodule, name\u001B[38;5;241m=\u001B[39mname)\n",
      "    \u001B[0;31m[... skipping similar frames: named_apply at line 34 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_manipulate.py:34\u001B[0m, in \u001B[0;36mnamed_apply\u001B[0;34m(fn, module, name, depth_first, include_root)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child_name, child_module \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m     33\u001B[0m     child_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin((name, child_name)) \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;28;01melse\u001B[39;00m child_name\n\u001B[0;32m---> 34\u001B[0m     \u001B[43mnamed_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdepth_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depth_first \u001B[38;5;129;01mand\u001B[39;00m include_root:\n\u001B[1;32m     36\u001B[0m     fn(module\u001B[38;5;241m=\u001B[39mmodule, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_manipulate.py:36\u001B[0m, in \u001B[0;36mnamed_apply\u001B[0;34m(fn, module, name, depth_first, include_root)\u001B[0m\n\u001B[1;32m     34\u001B[0m     named_apply(fn\u001B[38;5;241m=\u001B[39mfn, module\u001B[38;5;241m=\u001B[39mchild_module, name\u001B[38;5;241m=\u001B[39mchild_name, depth_first\u001B[38;5;241m=\u001B[39mdepth_first, include_root\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depth_first \u001B[38;5;129;01mand\u001B[39;00m include_root:\n\u001B[0;32m---> 36\u001B[0m     \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/vision_transformer.py:712\u001B[0m, in \u001B[0;36minit_weights_vit_timm\u001B[0;34m(module, name)\u001B[0m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\u001B[39;00m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module, nn\u001B[38;5;241m.\u001B[39mLinear):\n\u001B[0;32m--> 712\u001B[0m     \u001B[43mtrunc_normal_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.02\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    714\u001B[0m         nn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mzeros_(module\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/weight_init.py:67\u001B[0m, in \u001B[0;36mtrunc_normal_\u001B[0;34m(tensor, mean, std, a, b)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Fills the input Tensor with values drawn from a truncated\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03mnormal distribution. The values are effectively drawn from the\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    >>> nn.init.trunc_normal_(w)\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_trunc_normal_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/weight_init.py:28\u001B[0m, in \u001B[0;36m_trunc_normal_\u001B[0;34m(tensor, mean, std, a, b)\u001B[0m\n\u001B[1;32m     24\u001B[0m u \u001B[38;5;241m=\u001B[39m norm_cdf((b \u001B[38;5;241m-\u001B[39m mean) \u001B[38;5;241m/\u001B[39m std)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Uniformly fill tensor with values from [l, u], then translate to\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# [2l-1, 2u-1].\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43ml\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Use inverse cdf transform for normal distribution to get truncated\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# standard normal\u001B[39;00m\n\u001B[1;32m     32\u001B[0m tensor\u001B[38;5;241m.\u001B[39merfinv_()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "embeddings = []\n",
    "labels = []\n",
    "for i in tqdm(range(len(trainset))):\n",
    "    data, label = trainset[i]\n",
    "    embeddings.append(model(data.unsqueeze(0).to(device)).cpu().detach().numpy())\n",
    "    labels.append(label)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T20:45:19.426512Z",
     "iopub.execute_input": "2024-04-21T20:45:19.426859Z",
     "iopub.status.idle": "2024-04-21T20:59:21.940356Z",
     "shell.execute_reply.started": "2024-04-21T20:45:19.426833Z",
     "shell.execute_reply": "2024-04-21T20:59:21.939308Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/7010 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71021303fbcf4d39bf4d9c058a8d6382"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "np.squeeze(np.array(embeddings), 1).shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:01:12.940302Z",
     "iopub.execute_input": "2024-04-21T21:01:12.940994Z",
     "iopub.status.idle": "2024-04-21T21:01:12.958192Z",
     "shell.execute_reply.started": "2024-04-21T21:01:12.940960Z",
     "shell.execute_reply": "2024-04-21T21:01:12.957253Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 47,
   "outputs": [
    {
     "execution_count": 47,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7010, 768)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "\n",
    "clf.fit(np.squeeze(np.array(embeddings), 1), labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:01:20.957664Z",
     "iopub.execute_input": "2024-04-21T21:01:20.958754Z",
     "iopub.status.idle": "2024-04-21T21:01:32.490437Z",
     "shell.execute_reply.started": "2024-04-21T21:01:20.958708Z",
     "shell.execute_reply": "2024-04-21T21:01:32.489497Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 48,
   "outputs": [
    {
     "execution_count": 48,
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = clf.predict(np.squeeze(np.array(embeddings), 1))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:02:30.638051Z",
     "iopub.execute_input": "2024-04-21T21:02:30.638442Z",
     "iopub.status.idle": "2024-04-21T21:02:46.549882Z",
     "shell.execute_reply.started": "2024-04-21T21:02:30.638411Z",
     "shell.execute_reply": "2024-04-21T21:02:46.549029Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sum(predictions==np.array(labels))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:02:56.310500Z",
     "iopub.execute_input": "2024-04-21T21:02:56.311123Z",
     "iopub.status.idle": "2024-04-21T21:02:56.320674Z",
     "shell.execute_reply.started": "2024-04-21T21:02:56.311091Z",
     "shell.execute_reply": "2024-04-21T21:02:56.319640Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 51,
   "outputs": [
    {
     "execution_count": 51,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.846077032810271"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings_test = []\n",
    "labels_test = []\n",
    "for i in tqdm(range(len(valset))):\n",
    "    data, label = valset[i]\n",
    "    embeddings_test.append(model(data.unsqueeze(0).to(device)).cpu().detach().numpy())\n",
    "    labels_test.append(label)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:04:27.573201Z",
     "iopub.execute_input": "2024-04-21T21:04:27.574038Z",
     "iopub.status.idle": "2024-04-21T21:09:51.280344Z",
     "shell.execute_reply.started": "2024-04-21T21:04:27.574009Z",
     "shell.execute_reply": "2024-04-21T21:09:51.279327Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/3005 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d69e6d38f2b446839547e4741227eed6"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions_test = clf.predict(np.squeeze(np.array(embeddings_test), 1))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:17:00.845902Z",
     "iopub.execute_input": "2024-04-21T21:17:00.846304Z",
     "iopub.status.idle": "2024-04-21T21:17:07.325510Z",
     "shell.execute_reply.started": "2024-04-21T21:17:00.846274Z",
     "shell.execute_reply": "2024-04-21T21:17:07.324627Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sum(predictions_test==np.array(labels_test))/predictions_test.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-21T21:17:15.856778Z",
     "iopub.execute_input": "2024-04-21T21:17:15.857173Z",
     "iopub.status.idle": "2024-04-21T21:17:15.865171Z",
     "shell.execute_reply.started": "2024-04-21T21:17:15.857139Z",
     "shell.execute_reply": "2024-04-21T21:17:15.864134Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 54,
   "outputs": [
    {
     "execution_count": 54,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8066555740432613"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T04:20:46.421163Z",
     "iopub.execute_input": "2024-04-23T04:20:46.421872Z",
     "iopub.status.idle": "2024-04-23T04:20:53.151451Z",
     "shell.execute_reply.started": "2024-04-23T04:20:46.421841Z",
     "shell.execute_reply": "2024-04-23T04:20:53.150610Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Large dinov2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "trainset_dino = ClassificationDataset(metadata.iloc[train_idx][['path', 'dx_encoded']],\n",
    "                                 transform=transforms_test)\n",
    "train_loader_dino = torch.utils.data.DataLoader(trainset_dino, batch_size=64,shuffle=False,\n",
    "                                           collate_fn = collate_fn)\n",
    "\n",
    "for data, target in tqdm(train_loader_dino):\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings.extend(model(data).cpu().numpy())\n",
    "        \n",
    "    labels.extend(target)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T04:26:16.004429Z",
     "iopub.execute_input": "2024-04-23T04:26:16.004850Z",
     "iopub.status.idle": "2024-04-23T05:04:36.049788Z",
     "shell.execute_reply.started": "2024-04-23T04:26:16.004817Z",
     "shell.execute_reply": "2024-04-23T05:04:36.048798Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7480649886d74f459d3456b0d8b662a2"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "\n",
    "clf.fit(np.array(embeddings), labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:42:29.465927Z",
     "iopub.execute_input": "2024-04-23T05:42:29.466723Z",
     "iopub.status.idle": "2024-04-23T05:42:45.444547Z",
     "shell.execute_reply.started": "2024-04-23T05:42:29.466688Z",
     "shell.execute_reply": "2024-04-23T05:42:45.443550Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 35,
   "outputs": [
    {
     "execution_count": 35,
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC()",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = clf.predict(np.array(embeddings))\n",
    "sum(predictions==np.array(labels))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:06:59.991099Z",
     "iopub.execute_input": "2024-04-23T05:06:59.991446Z",
     "iopub.status.idle": "2024-04-23T05:07:22.527309Z",
     "shell.execute_reply.started": "2024-04-23T05:06:59.991420Z",
     "shell.execute_reply": "2024-04-23T05:07:22.526362Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8626248216833096"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions_val = clf.predict(np.array(embeddings_val))\n",
    "sum(predictions_val==np.array(labels_val))/predictions_val.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:44:49.258480Z",
     "iopub.execute_input": "2024-04-23T05:44:49.259371Z",
     "iopub.status.idle": "2024-04-23T05:44:58.293063Z",
     "shell.execute_reply.started": "2024-04-23T05:44:49.259336Z",
     "shell.execute_reply": "2024-04-23T05:44:58.292000Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "execution_count": 37,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8133111480865225"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostClassifier\n",
    "clf = CatBoostClassifier()\n",
    "\n",
    "clf.fit(np.array(embeddings), labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:09:19.500680Z",
     "iopub.execute_input": "2024-04-23T05:09:19.501565Z",
     "iopub.status.idle": "2024-04-23T05:22:22.962548Z",
     "shell.execute_reply.started": "2024-04-23T05:09:19.501521Z",
     "shell.execute_reply": "2024-04-23T05:22:22.961485Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": "Learning rate set to 0.087389\n0:\tlearn: 1.7132590\ttotal: 1.24s\tremaining: 20m 41s\n1:\tlearn: 1.5625689\ttotal: 2.08s\tremaining: 17m 17s\n2:\tlearn: 1.4502644\ttotal: 2.9s\tremaining: 16m 4s\n3:\tlearn: 1.3642465\ttotal: 3.72s\tremaining: 15m 27s\n4:\tlearn: 1.2939101\ttotal: 4.53s\tremaining: 15m 1s\n5:\tlearn: 1.2338379\ttotal: 5.34s\tremaining: 14m 44s\n6:\tlearn: 1.1852548\ttotal: 6.19s\tremaining: 14m 38s\n7:\tlearn: 1.1394666\ttotal: 7.03s\tremaining: 14m 31s\n8:\tlearn: 1.1001411\ttotal: 7.89s\tremaining: 14m 28s\n9:\tlearn: 1.0650195\ttotal: 8.72s\tremaining: 14m 23s\n10:\tlearn: 1.0333502\ttotal: 9.55s\tremaining: 14m 18s\n11:\tlearn: 1.0057806\ttotal: 10.4s\tremaining: 14m 15s\n12:\tlearn: 0.9822913\ttotal: 11.3s\tremaining: 14m 14s\n13:\tlearn: 0.9612714\ttotal: 12.1s\tremaining: 14m 11s\n14:\tlearn: 0.9407298\ttotal: 12.9s\tremaining: 14m 9s\n15:\tlearn: 0.9230102\ttotal: 13.8s\tremaining: 14m 7s\n16:\tlearn: 0.9048600\ttotal: 14.6s\tremaining: 14m 5s\n17:\tlearn: 0.8879767\ttotal: 15.5s\tremaining: 14m 3s\n18:\tlearn: 0.8722224\ttotal: 16.3s\tremaining: 14m 3s\n19:\tlearn: 0.8580241\ttotal: 17.1s\tremaining: 14m\n20:\tlearn: 0.8457866\ttotal: 17.9s\tremaining: 13m 56s\n21:\tlearn: 0.8324025\ttotal: 18.7s\tremaining: 13m 52s\n22:\tlearn: 0.8205303\ttotal: 19.6s\tremaining: 13m 51s\n23:\tlearn: 0.8083834\ttotal: 20.4s\tremaining: 13m 50s\n24:\tlearn: 0.7971593\ttotal: 21.2s\tremaining: 13m 48s\n25:\tlearn: 0.7873647\ttotal: 22s\tremaining: 13m 45s\n26:\tlearn: 0.7780972\ttotal: 23.1s\tremaining: 13m 54s\n27:\tlearn: 0.7693165\ttotal: 24.1s\tremaining: 13m 55s\n28:\tlearn: 0.7607840\ttotal: 24.9s\tremaining: 13m 52s\n29:\tlearn: 0.7523363\ttotal: 25.7s\tremaining: 13m 51s\n30:\tlearn: 0.7440268\ttotal: 26.6s\tremaining: 13m 49s\n31:\tlearn: 0.7375866\ttotal: 27.4s\tremaining: 13m 48s\n32:\tlearn: 0.7292951\ttotal: 28.2s\tremaining: 13m 45s\n33:\tlearn: 0.7216889\ttotal: 29s\tremaining: 13m 43s\n34:\tlearn: 0.7159483\ttotal: 29.8s\tremaining: 13m 41s\n35:\tlearn: 0.7098112\ttotal: 30.6s\tremaining: 13m 40s\n36:\tlearn: 0.7040783\ttotal: 31.5s\tremaining: 13m 38s\n37:\tlearn: 0.6978319\ttotal: 32.3s\tremaining: 13m 37s\n38:\tlearn: 0.6916314\ttotal: 33.2s\tremaining: 13m 37s\n39:\tlearn: 0.6869385\ttotal: 33.9s\tremaining: 13m 34s\n40:\tlearn: 0.6801267\ttotal: 34.7s\tremaining: 13m 32s\n41:\tlearn: 0.6755892\ttotal: 35.6s\tremaining: 13m 31s\n42:\tlearn: 0.6714496\ttotal: 36.4s\tremaining: 13m 29s\n43:\tlearn: 0.6661743\ttotal: 37.2s\tremaining: 13m 28s\n44:\tlearn: 0.6619579\ttotal: 38s\tremaining: 13m 26s\n45:\tlearn: 0.6576197\ttotal: 38.9s\tremaining: 13m 25s\n46:\tlearn: 0.6522656\ttotal: 39.7s\tremaining: 13m 25s\n47:\tlearn: 0.6477679\ttotal: 40.5s\tremaining: 13m 23s\n48:\tlearn: 0.6444156\ttotal: 41.4s\tremaining: 13m 22s\n49:\tlearn: 0.6398862\ttotal: 42.2s\tremaining: 13m 20s\n50:\tlearn: 0.6358501\ttotal: 43s\tremaining: 13m 19s\n51:\tlearn: 0.6317110\ttotal: 43.8s\tremaining: 13m 18s\n52:\tlearn: 0.6280672\ttotal: 44.6s\tremaining: 13m 17s\n53:\tlearn: 0.6251239\ttotal: 45.4s\tremaining: 13m 15s\n54:\tlearn: 0.6216818\ttotal: 46.3s\tremaining: 13m 14s\n55:\tlearn: 0.6172614\ttotal: 47.1s\tremaining: 13m 13s\n56:\tlearn: 0.6128660\ttotal: 47.9s\tremaining: 13m 12s\n57:\tlearn: 0.6089563\ttotal: 48.7s\tremaining: 13m 11s\n58:\tlearn: 0.6055587\ttotal: 49.5s\tremaining: 13m 9s\n59:\tlearn: 0.6024965\ttotal: 50.3s\tremaining: 13m 8s\n60:\tlearn: 0.5997550\ttotal: 51.1s\tremaining: 13m 7s\n61:\tlearn: 0.5967775\ttotal: 51.9s\tremaining: 13m 5s\n62:\tlearn: 0.5929243\ttotal: 52.8s\tremaining: 13m 4s\n63:\tlearn: 0.5887179\ttotal: 53.6s\tremaining: 13m 3s\n64:\tlearn: 0.5851551\ttotal: 54.6s\tremaining: 13m 5s\n65:\tlearn: 0.5813059\ttotal: 55.6s\tremaining: 13m 6s\n66:\tlearn: 0.5786768\ttotal: 56.4s\tremaining: 13m 5s\n67:\tlearn: 0.5747658\ttotal: 57.2s\tremaining: 13m 3s\n68:\tlearn: 0.5713085\ttotal: 58s\tremaining: 13m 2s\n69:\tlearn: 0.5685430\ttotal: 58.8s\tremaining: 13m 1s\n70:\tlearn: 0.5656886\ttotal: 59.6s\tremaining: 12m 59s\n71:\tlearn: 0.5619365\ttotal: 1m\tremaining: 12m 58s\n72:\tlearn: 0.5598906\ttotal: 1m 1s\tremaining: 12m 56s\n73:\tlearn: 0.5562651\ttotal: 1m 2s\tremaining: 12m 55s\n74:\tlearn: 0.5527046\ttotal: 1m 2s\tremaining: 12m 55s\n75:\tlearn: 0.5502722\ttotal: 1m 3s\tremaining: 12m 54s\n76:\tlearn: 0.5477800\ttotal: 1m 4s\tremaining: 12m 52s\n77:\tlearn: 0.5445900\ttotal: 1m 5s\tremaining: 12m 51s\n78:\tlearn: 0.5415492\ttotal: 1m 6s\tremaining: 12m 50s\n79:\tlearn: 0.5384456\ttotal: 1m 6s\tremaining: 12m 50s\n80:\tlearn: 0.5357799\ttotal: 1m 7s\tremaining: 12m 49s\n81:\tlearn: 0.5333111\ttotal: 1m 8s\tremaining: 12m 47s\n82:\tlearn: 0.5309067\ttotal: 1m 9s\tremaining: 12m 46s\n83:\tlearn: 0.5286606\ttotal: 1m 10s\tremaining: 12m 45s\n84:\tlearn: 0.5254458\ttotal: 1m 10s\tremaining: 12m 44s\n85:\tlearn: 0.5226065\ttotal: 1m 11s\tremaining: 12m 42s\n86:\tlearn: 0.5201458\ttotal: 1m 12s\tremaining: 12m 41s\n87:\tlearn: 0.5173023\ttotal: 1m 13s\tremaining: 12m 40s\n88:\tlearn: 0.5149501\ttotal: 1m 14s\tremaining: 12m 39s\n89:\tlearn: 0.5121027\ttotal: 1m 15s\tremaining: 12m 38s\n90:\tlearn: 0.5102304\ttotal: 1m 15s\tremaining: 12m 37s\n91:\tlearn: 0.5086220\ttotal: 1m 16s\tremaining: 12m 36s\n92:\tlearn: 0.5066821\ttotal: 1m 17s\tremaining: 12m 35s\n93:\tlearn: 0.5048456\ttotal: 1m 18s\tremaining: 12m 33s\n94:\tlearn: 0.5027047\ttotal: 1m 18s\tremaining: 12m 32s\n95:\tlearn: 0.5009210\ttotal: 1m 19s\tremaining: 12m 31s\n96:\tlearn: 0.5000841\ttotal: 1m 20s\tremaining: 12m 29s\n97:\tlearn: 0.4981037\ttotal: 1m 21s\tremaining: 12m 28s\n98:\tlearn: 0.4955242\ttotal: 1m 22s\tremaining: 12m 27s\n99:\tlearn: 0.4933519\ttotal: 1m 22s\tremaining: 12m 26s\n100:\tlearn: 0.4916709\ttotal: 1m 23s\tremaining: 12m 24s\n101:\tlearn: 0.4892772\ttotal: 1m 24s\tremaining: 12m 23s\n102:\tlearn: 0.4864720\ttotal: 1m 25s\tremaining: 12m 22s\n103:\tlearn: 0.4843696\ttotal: 1m 26s\tremaining: 12m 23s\n104:\tlearn: 0.4826275\ttotal: 1m 27s\tremaining: 12m 24s\n105:\tlearn: 0.4809670\ttotal: 1m 28s\tremaining: 12m 23s\n106:\tlearn: 0.4796790\ttotal: 1m 28s\tremaining: 12m 21s\n107:\tlearn: 0.4769783\ttotal: 1m 29s\tremaining: 12m 21s\n108:\tlearn: 0.4751451\ttotal: 1m 30s\tremaining: 12m 19s\n109:\tlearn: 0.4727417\ttotal: 1m 31s\tremaining: 12m 18s\n110:\tlearn: 0.4705444\ttotal: 1m 32s\tremaining: 12m 17s\n111:\tlearn: 0.4684105\ttotal: 1m 32s\tremaining: 12m 17s\n112:\tlearn: 0.4667053\ttotal: 1m 33s\tremaining: 12m 16s\n113:\tlearn: 0.4648900\ttotal: 1m 34s\tremaining: 12m 14s\n114:\tlearn: 0.4636690\ttotal: 1m 35s\tremaining: 12m 13s\n115:\tlearn: 0.4620767\ttotal: 1m 36s\tremaining: 12m 12s\n116:\tlearn: 0.4598553\ttotal: 1m 36s\tremaining: 12m 11s\n117:\tlearn: 0.4580798\ttotal: 1m 37s\tremaining: 12m 10s\n118:\tlearn: 0.4553136\ttotal: 1m 38s\tremaining: 12m 9s\n119:\tlearn: 0.4534622\ttotal: 1m 39s\tremaining: 12m 8s\n120:\tlearn: 0.4522712\ttotal: 1m 40s\tremaining: 12m 7s\n121:\tlearn: 0.4502652\ttotal: 1m 40s\tremaining: 12m 5s\n122:\tlearn: 0.4477356\ttotal: 1m 41s\tremaining: 12m 4s\n123:\tlearn: 0.4462628\ttotal: 1m 42s\tremaining: 12m 3s\n124:\tlearn: 0.4446568\ttotal: 1m 43s\tremaining: 12m 2s\n125:\tlearn: 0.4425277\ttotal: 1m 43s\tremaining: 12m 1s\n126:\tlearn: 0.4409930\ttotal: 1m 44s\tremaining: 12m\n127:\tlearn: 0.4392091\ttotal: 1m 45s\tremaining: 11m 58s\n128:\tlearn: 0.4377935\ttotal: 1m 46s\tremaining: 11m 58s\n129:\tlearn: 0.4361234\ttotal: 1m 47s\tremaining: 11m 56s\n130:\tlearn: 0.4339348\ttotal: 1m 47s\tremaining: 11m 55s\n131:\tlearn: 0.4318145\ttotal: 1m 48s\tremaining: 11m 54s\n132:\tlearn: 0.4300265\ttotal: 1m 49s\tremaining: 11m 53s\n133:\tlearn: 0.4281769\ttotal: 1m 50s\tremaining: 11m 52s\n134:\tlearn: 0.4270673\ttotal: 1m 51s\tremaining: 11m 51s\n135:\tlearn: 0.4258330\ttotal: 1m 51s\tremaining: 11m 50s\n136:\tlearn: 0.4241599\ttotal: 1m 52s\tremaining: 11m 49s\n137:\tlearn: 0.4230777\ttotal: 1m 53s\tremaining: 11m 48s\n138:\tlearn: 0.4212979\ttotal: 1m 54s\tremaining: 11m 46s\n139:\tlearn: 0.4196808\ttotal: 1m 54s\tremaining: 11m 45s\n140:\tlearn: 0.4184542\ttotal: 1m 55s\tremaining: 11m 44s\n141:\tlearn: 0.4170600\ttotal: 1m 56s\tremaining: 11m 43s\n142:\tlearn: 0.4158390\ttotal: 1m 57s\tremaining: 11m 42s\n143:\tlearn: 0.4147984\ttotal: 1m 58s\tremaining: 11m 42s\n144:\tlearn: 0.4137159\ttotal: 1m 59s\tremaining: 11m 42s\n145:\tlearn: 0.4124049\ttotal: 1m 59s\tremaining: 11m 41s\n146:\tlearn: 0.4111309\ttotal: 2m\tremaining: 11m 40s\n147:\tlearn: 0.4100130\ttotal: 2m 1s\tremaining: 11m 39s\n148:\tlearn: 0.4087684\ttotal: 2m 2s\tremaining: 11m 38s\n149:\tlearn: 0.4072972\ttotal: 2m 3s\tremaining: 11m 37s\n150:\tlearn: 0.4058446\ttotal: 2m 3s\tremaining: 11m 36s\n151:\tlearn: 0.4049564\ttotal: 2m 4s\tremaining: 11m 35s\n152:\tlearn: 0.4031922\ttotal: 2m 5s\tremaining: 11m 33s\n153:\tlearn: 0.4019737\ttotal: 2m 6s\tremaining: 11m 32s\n154:\tlearn: 0.4006713\ttotal: 2m 6s\tremaining: 11m 32s\n155:\tlearn: 0.3996318\ttotal: 2m 7s\tremaining: 11m 30s\n156:\tlearn: 0.3982134\ttotal: 2m 8s\tremaining: 11m 29s\n157:\tlearn: 0.3965067\ttotal: 2m 9s\tremaining: 11m 28s\n158:\tlearn: 0.3955204\ttotal: 2m 10s\tremaining: 11m 27s\n159:\tlearn: 0.3944240\ttotal: 2m 10s\tremaining: 11m 26s\n160:\tlearn: 0.3934209\ttotal: 2m 11s\tremaining: 11m 25s\n161:\tlearn: 0.3920878\ttotal: 2m 12s\tremaining: 11m 24s\n162:\tlearn: 0.3906034\ttotal: 2m 13s\tremaining: 11m 23s\n163:\tlearn: 0.3888741\ttotal: 2m 13s\tremaining: 11m 23s\n164:\tlearn: 0.3872290\ttotal: 2m 14s\tremaining: 11m 22s\n165:\tlearn: 0.3861200\ttotal: 2m 15s\tremaining: 11m 21s\n166:\tlearn: 0.3851494\ttotal: 2m 16s\tremaining: 11m 20s\n167:\tlearn: 0.3834748\ttotal: 2m 17s\tremaining: 11m 19s\n168:\tlearn: 0.3824077\ttotal: 2m 17s\tremaining: 11m 18s\n169:\tlearn: 0.3813696\ttotal: 2m 18s\tremaining: 11m 17s\n170:\tlearn: 0.3801508\ttotal: 2m 19s\tremaining: 11m 16s\n171:\tlearn: 0.3792677\ttotal: 2m 20s\tremaining: 11m 15s\n172:\tlearn: 0.3782712\ttotal: 2m 21s\tremaining: 11m 14s\n173:\tlearn: 0.3775125\ttotal: 2m 21s\tremaining: 11m 13s\n174:\tlearn: 0.3766307\ttotal: 2m 22s\tremaining: 11m 12s\n175:\tlearn: 0.3751991\ttotal: 2m 23s\tremaining: 11m 11s\n176:\tlearn: 0.3733372\ttotal: 2m 24s\tremaining: 11m 10s\n177:\tlearn: 0.3726814\ttotal: 2m 24s\tremaining: 11m 9s\n178:\tlearn: 0.3713690\ttotal: 2m 25s\tremaining: 11m 8s\n179:\tlearn: 0.3708708\ttotal: 2m 26s\tremaining: 11m 7s\n180:\tlearn: 0.3701124\ttotal: 2m 27s\tremaining: 11m 6s\n181:\tlearn: 0.3690785\ttotal: 2m 28s\tremaining: 11m 5s\n182:\tlearn: 0.3677820\ttotal: 2m 28s\tremaining: 11m 4s\n183:\tlearn: 0.3667465\ttotal: 2m 29s\tremaining: 11m 3s\n184:\tlearn: 0.3659235\ttotal: 2m 30s\tremaining: 11m 3s\n185:\tlearn: 0.3648149\ttotal: 2m 31s\tremaining: 11m 3s\n186:\tlearn: 0.3637887\ttotal: 2m 32s\tremaining: 11m 2s\n187:\tlearn: 0.3628451\ttotal: 2m 33s\tremaining: 11m 1s\n188:\tlearn: 0.3617264\ttotal: 2m 33s\tremaining: 11m\n189:\tlearn: 0.3608930\ttotal: 2m 34s\tremaining: 10m 59s\n190:\tlearn: 0.3601860\ttotal: 2m 35s\tremaining: 10m 58s\n191:\tlearn: 0.3593846\ttotal: 2m 36s\tremaining: 10m 57s\n192:\tlearn: 0.3582706\ttotal: 2m 36s\tremaining: 10m 56s\n193:\tlearn: 0.3574639\ttotal: 2m 37s\tremaining: 10m 55s\n194:\tlearn: 0.3561880\ttotal: 2m 38s\tremaining: 10m 54s\n195:\tlearn: 0.3554755\ttotal: 2m 39s\tremaining: 10m 53s\n196:\tlearn: 0.3543494\ttotal: 2m 40s\tremaining: 10m 52s\n197:\tlearn: 0.3535002\ttotal: 2m 40s\tremaining: 10m 51s\n198:\tlearn: 0.3527017\ttotal: 2m 41s\tremaining: 10m 50s\n199:\tlearn: 0.3517242\ttotal: 2m 42s\tremaining: 10m 49s\n200:\tlearn: 0.3507892\ttotal: 2m 43s\tremaining: 10m 48s\n201:\tlearn: 0.3496998\ttotal: 2m 43s\tremaining: 10m 47s\n202:\tlearn: 0.3486805\ttotal: 2m 44s\tremaining: 10m 46s\n203:\tlearn: 0.3478246\ttotal: 2m 45s\tremaining: 10m 45s\n204:\tlearn: 0.3471380\ttotal: 2m 46s\tremaining: 10m 44s\n205:\tlearn: 0.3461784\ttotal: 2m 46s\tremaining: 10m 43s\n206:\tlearn: 0.3450438\ttotal: 2m 47s\tremaining: 10m 42s\n207:\tlearn: 0.3438892\ttotal: 2m 48s\tremaining: 10m 41s\n208:\tlearn: 0.3427849\ttotal: 2m 49s\tremaining: 10m 40s\n209:\tlearn: 0.3420107\ttotal: 2m 49s\tremaining: 10m 39s\n210:\tlearn: 0.3411072\ttotal: 2m 50s\tremaining: 10m 38s\n211:\tlearn: 0.3402834\ttotal: 2m 51s\tremaining: 10m 37s\n212:\tlearn: 0.3393775\ttotal: 2m 52s\tremaining: 10m 36s\n213:\tlearn: 0.3385648\ttotal: 2m 53s\tremaining: 10m 35s\n214:\tlearn: 0.3379442\ttotal: 2m 53s\tremaining: 10m 34s\n215:\tlearn: 0.3366726\ttotal: 2m 54s\tremaining: 10m 33s\n216:\tlearn: 0.3359880\ttotal: 2m 55s\tremaining: 10m 32s\n217:\tlearn: 0.3351506\ttotal: 2m 56s\tremaining: 10m 31s\n218:\tlearn: 0.3340557\ttotal: 2m 56s\tremaining: 10m 30s\n219:\tlearn: 0.3332665\ttotal: 2m 57s\tremaining: 10m 30s\n220:\tlearn: 0.3322521\ttotal: 2m 58s\tremaining: 10m 29s\n221:\tlearn: 0.3314093\ttotal: 2m 59s\tremaining: 10m 28s\n222:\tlearn: 0.3308061\ttotal: 2m 59s\tremaining: 10m 27s\n223:\tlearn: 0.3297894\ttotal: 3m\tremaining: 10m 26s\n224:\tlearn: 0.3290923\ttotal: 3m 1s\tremaining: 10m 25s\n225:\tlearn: 0.3284273\ttotal: 3m 2s\tremaining: 10m 25s\n226:\tlearn: 0.3276721\ttotal: 3m 3s\tremaining: 10m 24s\n227:\tlearn: 0.3269022\ttotal: 3m 4s\tremaining: 10m 23s\n228:\tlearn: 0.3258772\ttotal: 3m 4s\tremaining: 10m 22s\n229:\tlearn: 0.3252311\ttotal: 3m 5s\tremaining: 10m 21s\n230:\tlearn: 0.3242685\ttotal: 3m 6s\tremaining: 10m 20s\n231:\tlearn: 0.3236068\ttotal: 3m 7s\tremaining: 10m 19s\n232:\tlearn: 0.3228681\ttotal: 3m 8s\tremaining: 10m 19s\n233:\tlearn: 0.3221569\ttotal: 3m 8s\tremaining: 10m 18s\n234:\tlearn: 0.3211068\ttotal: 3m 9s\tremaining: 10m 17s\n235:\tlearn: 0.3203638\ttotal: 3m 10s\tremaining: 10m 16s\n236:\tlearn: 0.3195452\ttotal: 3m 11s\tremaining: 10m 15s\n237:\tlearn: 0.3189664\ttotal: 3m 11s\tremaining: 10m 14s\n238:\tlearn: 0.3182941\ttotal: 3m 12s\tremaining: 10m 13s\n239:\tlearn: 0.3177639\ttotal: 3m 13s\tremaining: 10m 12s\n240:\tlearn: 0.3171044\ttotal: 3m 14s\tremaining: 10m 11s\n241:\tlearn: 0.3163062\ttotal: 3m 14s\tremaining: 10m 10s\n242:\tlearn: 0.3152885\ttotal: 3m 15s\tremaining: 10m 9s\n243:\tlearn: 0.3144429\ttotal: 3m 16s\tremaining: 10m 8s\n244:\tlearn: 0.3137617\ttotal: 3m 17s\tremaining: 10m 7s\n245:\tlearn: 0.3134047\ttotal: 3m 17s\tremaining: 10m 6s\n246:\tlearn: 0.3125726\ttotal: 3m 18s\tremaining: 10m 5s\n247:\tlearn: 0.3119549\ttotal: 3m 19s\tremaining: 10m 4s\n248:\tlearn: 0.3112990\ttotal: 3m 20s\tremaining: 10m 3s\n249:\tlearn: 0.3103772\ttotal: 3m 20s\tremaining: 10m 2s\n250:\tlearn: 0.3095200\ttotal: 3m 21s\tremaining: 10m 2s\n251:\tlearn: 0.3089387\ttotal: 3m 22s\tremaining: 10m 1s\n252:\tlearn: 0.3084161\ttotal: 3m 23s\tremaining: 10m\n253:\tlearn: 0.3077755\ttotal: 3m 24s\tremaining: 9m 59s\n254:\tlearn: 0.3071437\ttotal: 3m 24s\tremaining: 9m 58s\n255:\tlearn: 0.3061998\ttotal: 3m 25s\tremaining: 9m 57s\n256:\tlearn: 0.3053526\ttotal: 3m 26s\tremaining: 9m 56s\n257:\tlearn: 0.3044187\ttotal: 3m 27s\tremaining: 9m 56s\n258:\tlearn: 0.3037551\ttotal: 3m 28s\tremaining: 9m 55s\n259:\tlearn: 0.3026723\ttotal: 3m 28s\tremaining: 9m 54s\n260:\tlearn: 0.3017547\ttotal: 3m 29s\tremaining: 9m 53s\n261:\tlearn: 0.3011336\ttotal: 3m 30s\tremaining: 9m 52s\n262:\tlearn: 0.3005118\ttotal: 3m 31s\tremaining: 9m 51s\n263:\tlearn: 0.2998402\ttotal: 3m 31s\tremaining: 9m 50s\n264:\tlearn: 0.2983399\ttotal: 3m 32s\tremaining: 9m 50s\n265:\tlearn: 0.2975897\ttotal: 3m 33s\tremaining: 9m 49s\n266:\tlearn: 0.2969789\ttotal: 3m 34s\tremaining: 9m 49s\n267:\tlearn: 0.2963803\ttotal: 3m 35s\tremaining: 9m 48s\n268:\tlearn: 0.2956925\ttotal: 3m 36s\tremaining: 9m 47s\n269:\tlearn: 0.2951638\ttotal: 3m 36s\tremaining: 9m 46s\n270:\tlearn: 0.2941203\ttotal: 3m 37s\tremaining: 9m 45s\n271:\tlearn: 0.2933704\ttotal: 3m 38s\tremaining: 9m 45s\n272:\tlearn: 0.2927878\ttotal: 3m 39s\tremaining: 9m 44s\n273:\tlearn: 0.2924603\ttotal: 3m 40s\tremaining: 9m 43s\n274:\tlearn: 0.2915432\ttotal: 3m 40s\tremaining: 9m 42s\n275:\tlearn: 0.2909112\ttotal: 3m 41s\tremaining: 9m 41s\n276:\tlearn: 0.2899500\ttotal: 3m 42s\tremaining: 9m 40s\n277:\tlearn: 0.2891249\ttotal: 3m 43s\tremaining: 9m 39s\n278:\tlearn: 0.2885831\ttotal: 3m 43s\tremaining: 9m 38s\n279:\tlearn: 0.2879137\ttotal: 3m 44s\tremaining: 9m 37s\n280:\tlearn: 0.2873963\ttotal: 3m 45s\tremaining: 9m 36s\n281:\tlearn: 0.2868710\ttotal: 3m 46s\tremaining: 9m 36s\n282:\tlearn: 0.2862773\ttotal: 3m 47s\tremaining: 9m 35s\n283:\tlearn: 0.2853452\ttotal: 3m 47s\tremaining: 9m 34s\n284:\tlearn: 0.2846485\ttotal: 3m 48s\tremaining: 9m 33s\n285:\tlearn: 0.2839157\ttotal: 3m 49s\tremaining: 9m 32s\n286:\tlearn: 0.2832225\ttotal: 3m 50s\tremaining: 9m 31s\n287:\tlearn: 0.2825633\ttotal: 3m 50s\tremaining: 9m 30s\n288:\tlearn: 0.2818058\ttotal: 3m 51s\tremaining: 9m 29s\n289:\tlearn: 0.2810635\ttotal: 3m 52s\tremaining: 9m 29s\n290:\tlearn: 0.2806670\ttotal: 3m 53s\tremaining: 9m 28s\n291:\tlearn: 0.2800476\ttotal: 3m 53s\tremaining: 9m 27s\n292:\tlearn: 0.2795051\ttotal: 3m 54s\tremaining: 9m 26s\n293:\tlearn: 0.2788701\ttotal: 3m 55s\tremaining: 9m 25s\n294:\tlearn: 0.2780999\ttotal: 3m 56s\tremaining: 9m 24s\n295:\tlearn: 0.2775476\ttotal: 3m 57s\tremaining: 9m 23s\n296:\tlearn: 0.2768719\ttotal: 3m 57s\tremaining: 9m 23s\n297:\tlearn: 0.2760913\ttotal: 3m 58s\tremaining: 9m 22s\n298:\tlearn: 0.2753416\ttotal: 3m 59s\tremaining: 9m 21s\n299:\tlearn: 0.2745218\ttotal: 4m\tremaining: 9m 20s\n300:\tlearn: 0.2736276\ttotal: 4m\tremaining: 9m 19s\n301:\tlearn: 0.2733025\ttotal: 4m 1s\tremaining: 9m 18s\n302:\tlearn: 0.2728834\ttotal: 4m 2s\tremaining: 9m 17s\n303:\tlearn: 0.2722045\ttotal: 4m 3s\tremaining: 9m 16s\n304:\tlearn: 0.2713049\ttotal: 4m 4s\tremaining: 9m 16s\n305:\tlearn: 0.2706488\ttotal: 4m 4s\tremaining: 9m 15s\n306:\tlearn: 0.2701826\ttotal: 4m 5s\tremaining: 9m 14s\n307:\tlearn: 0.2696741\ttotal: 4m 6s\tremaining: 9m 14s\n308:\tlearn: 0.2689421\ttotal: 4m 7s\tremaining: 9m 13s\n309:\tlearn: 0.2685420\ttotal: 4m 8s\tremaining: 9m 12s\n310:\tlearn: 0.2678310\ttotal: 4m 9s\tremaining: 9m 11s\n311:\tlearn: 0.2669025\ttotal: 4m 9s\tremaining: 9m 11s\n312:\tlearn: 0.2664125\ttotal: 4m 10s\tremaining: 9m 10s\n313:\tlearn: 0.2658887\ttotal: 4m 11s\tremaining: 9m 9s\n314:\tlearn: 0.2649950\ttotal: 4m 12s\tremaining: 9m 8s\n315:\tlearn: 0.2642134\ttotal: 4m 13s\tremaining: 9m 7s\n316:\tlearn: 0.2638385\ttotal: 4m 13s\tremaining: 9m 6s\n317:\tlearn: 0.2633109\ttotal: 4m 14s\tremaining: 9m 6s\n318:\tlearn: 0.2629497\ttotal: 4m 15s\tremaining: 9m 5s\n319:\tlearn: 0.2623799\ttotal: 4m 16s\tremaining: 9m 4s\n320:\tlearn: 0.2619533\ttotal: 4m 16s\tremaining: 9m 3s\n321:\tlearn: 0.2614143\ttotal: 4m 17s\tremaining: 9m 2s\n322:\tlearn: 0.2609519\ttotal: 4m 18s\tremaining: 9m 1s\n323:\tlearn: 0.2604916\ttotal: 4m 19s\tremaining: 9m\n324:\tlearn: 0.2597399\ttotal: 4m 19s\tremaining: 8m 59s\n325:\tlearn: 0.2591001\ttotal: 4m 20s\tremaining: 8m 59s\n326:\tlearn: 0.2584978\ttotal: 4m 21s\tremaining: 8m 58s\n327:\tlearn: 0.2578244\ttotal: 4m 22s\tremaining: 8m 57s\n328:\tlearn: 0.2570995\ttotal: 4m 23s\tremaining: 8m 56s\n329:\tlearn: 0.2563774\ttotal: 4m 23s\tremaining: 8m 55s\n330:\tlearn: 0.2559452\ttotal: 4m 24s\tremaining: 8m 54s\n331:\tlearn: 0.2554094\ttotal: 4m 25s\tremaining: 8m 53s\n332:\tlearn: 0.2548317\ttotal: 4m 26s\tremaining: 8m 53s\n333:\tlearn: 0.2542934\ttotal: 4m 26s\tremaining: 8m 52s\n334:\tlearn: 0.2538497\ttotal: 4m 27s\tremaining: 8m 51s\n335:\tlearn: 0.2534785\ttotal: 4m 28s\tremaining: 8m 50s\n336:\tlearn: 0.2530632\ttotal: 4m 29s\tremaining: 8m 49s\n337:\tlearn: 0.2525638\ttotal: 4m 29s\tremaining: 8m 48s\n338:\tlearn: 0.2519500\ttotal: 4m 30s\tremaining: 8m 47s\n339:\tlearn: 0.2514391\ttotal: 4m 31s\tremaining: 8m 47s\n340:\tlearn: 0.2506573\ttotal: 4m 32s\tremaining: 8m 46s\n341:\tlearn: 0.2500560\ttotal: 4m 33s\tremaining: 8m 45s\n342:\tlearn: 0.2495640\ttotal: 4m 33s\tremaining: 8m 44s\n343:\tlearn: 0.2489355\ttotal: 4m 34s\tremaining: 8m 43s\n344:\tlearn: 0.2486643\ttotal: 4m 35s\tremaining: 8m 42s\n345:\tlearn: 0.2482805\ttotal: 4m 36s\tremaining: 8m 41s\n346:\tlearn: 0.2472370\ttotal: 4m 37s\tremaining: 8m 41s\n347:\tlearn: 0.2467437\ttotal: 4m 37s\tremaining: 8m 40s\n348:\tlearn: 0.2463037\ttotal: 4m 38s\tremaining: 8m 40s\n349:\tlearn: 0.2458804\ttotal: 4m 39s\tremaining: 8m 39s\n350:\tlearn: 0.2455548\ttotal: 4m 40s\tremaining: 8m 38s\n351:\tlearn: 0.2450216\ttotal: 4m 41s\tremaining: 8m 37s\n352:\tlearn: 0.2447438\ttotal: 4m 41s\tremaining: 8m 36s\n353:\tlearn: 0.2443482\ttotal: 4m 42s\tremaining: 8m 35s\n354:\tlearn: 0.2439407\ttotal: 4m 43s\tremaining: 8m 35s\n355:\tlearn: 0.2435298\ttotal: 4m 44s\tremaining: 8m 34s\n356:\tlearn: 0.2429607\ttotal: 4m 44s\tremaining: 8m 33s\n357:\tlearn: 0.2425783\ttotal: 4m 45s\tremaining: 8m 32s\n358:\tlearn: 0.2422856\ttotal: 4m 46s\tremaining: 8m 31s\n359:\tlearn: 0.2418009\ttotal: 4m 47s\tremaining: 8m 30s\n360:\tlearn: 0.2411743\ttotal: 4m 48s\tremaining: 8m 29s\n361:\tlearn: 0.2408430\ttotal: 4m 48s\tremaining: 8m 29s\n362:\tlearn: 0.2403871\ttotal: 4m 49s\tremaining: 8m 28s\n363:\tlearn: 0.2399737\ttotal: 4m 50s\tremaining: 8m 27s\n364:\tlearn: 0.2392805\ttotal: 4m 51s\tremaining: 8m 26s\n365:\tlearn: 0.2388536\ttotal: 4m 51s\tremaining: 8m 25s\n366:\tlearn: 0.2383547\ttotal: 4m 52s\tremaining: 8m 24s\n367:\tlearn: 0.2378499\ttotal: 4m 53s\tremaining: 8m 24s\n368:\tlearn: 0.2373775\ttotal: 4m 54s\tremaining: 8m 23s\n369:\tlearn: 0.2367517\ttotal: 4m 55s\tremaining: 8m 22s\n370:\tlearn: 0.2361999\ttotal: 4m 55s\tremaining: 8m 21s\n371:\tlearn: 0.2358952\ttotal: 4m 56s\tremaining: 8m 20s\n372:\tlearn: 0.2355094\ttotal: 4m 57s\tremaining: 8m 19s\n373:\tlearn: 0.2349853\ttotal: 4m 58s\tremaining: 8m 19s\n374:\tlearn: 0.2345361\ttotal: 4m 58s\tremaining: 8m 18s\n375:\tlearn: 0.2342389\ttotal: 4m 59s\tremaining: 8m 17s\n376:\tlearn: 0.2337183\ttotal: 5m\tremaining: 8m 16s\n377:\tlearn: 0.2334154\ttotal: 5m 1s\tremaining: 8m 15s\n378:\tlearn: 0.2328454\ttotal: 5m 1s\tremaining: 8m 14s\n379:\tlearn: 0.2324954\ttotal: 5m 2s\tremaining: 8m 13s\n380:\tlearn: 0.2319059\ttotal: 5m 3s\tremaining: 8m 13s\n381:\tlearn: 0.2313674\ttotal: 5m 4s\tremaining: 8m 12s\n382:\tlearn: 0.2309080\ttotal: 5m 5s\tremaining: 8m 11s\n383:\tlearn: 0.2306034\ttotal: 5m 5s\tremaining: 8m 10s\n384:\tlearn: 0.2302965\ttotal: 5m 6s\tremaining: 8m 9s\n385:\tlearn: 0.2298712\ttotal: 5m 7s\tremaining: 8m 8s\n386:\tlearn: 0.2294397\ttotal: 5m 8s\tremaining: 8m 8s\n387:\tlearn: 0.2291023\ttotal: 5m 8s\tremaining: 8m 7s\n388:\tlearn: 0.2286468\ttotal: 5m 9s\tremaining: 8m 6s\n389:\tlearn: 0.2281772\ttotal: 5m 10s\tremaining: 8m 6s\n390:\tlearn: 0.2277982\ttotal: 5m 11s\tremaining: 8m 5s\n391:\tlearn: 0.2274052\ttotal: 5m 12s\tremaining: 8m 4s\n392:\tlearn: 0.2269518\ttotal: 5m 13s\tremaining: 8m 3s\n393:\tlearn: 0.2265550\ttotal: 5m 13s\tremaining: 8m 2s\n394:\tlearn: 0.2263355\ttotal: 5m 14s\tremaining: 8m 1s\n395:\tlearn: 0.2260458\ttotal: 5m 15s\tremaining: 8m 1s\n396:\tlearn: 0.2255804\ttotal: 5m 16s\tremaining: 8m\n397:\tlearn: 0.2253735\ttotal: 5m 16s\tremaining: 7m 59s\n398:\tlearn: 0.2248876\ttotal: 5m 17s\tremaining: 7m 58s\n399:\tlearn: 0.2244023\ttotal: 5m 18s\tremaining: 7m 57s\n400:\tlearn: 0.2239716\ttotal: 5m 19s\tremaining: 7m 57s\n401:\tlearn: 0.2236168\ttotal: 5m 20s\tremaining: 7m 56s\n402:\tlearn: 0.2232320\ttotal: 5m 20s\tremaining: 7m 55s\n403:\tlearn: 0.2227338\ttotal: 5m 21s\tremaining: 7m 54s\n404:\tlearn: 0.2220370\ttotal: 5m 22s\tremaining: 7m 53s\n405:\tlearn: 0.2216897\ttotal: 5m 23s\tremaining: 7m 52s\n406:\tlearn: 0.2210741\ttotal: 5m 24s\tremaining: 7m 52s\n407:\tlearn: 0.2207291\ttotal: 5m 24s\tremaining: 7m 51s\n408:\tlearn: 0.2204300\ttotal: 5m 25s\tremaining: 7m 50s\n409:\tlearn: 0.2200318\ttotal: 5m 26s\tremaining: 7m 49s\n410:\tlearn: 0.2196616\ttotal: 5m 27s\tremaining: 7m 48s\n411:\tlearn: 0.2192104\ttotal: 5m 27s\tremaining: 7m 48s\n412:\tlearn: 0.2187683\ttotal: 5m 28s\tremaining: 7m 47s\n413:\tlearn: 0.2186020\ttotal: 5m 29s\tremaining: 7m 46s\n414:\tlearn: 0.2178941\ttotal: 5m 30s\tremaining: 7m 45s\n415:\tlearn: 0.2175385\ttotal: 5m 31s\tremaining: 7m 44s\n416:\tlearn: 0.2172028\ttotal: 5m 31s\tremaining: 7m 43s\n417:\tlearn: 0.2168197\ttotal: 5m 32s\tremaining: 7m 43s\n418:\tlearn: 0.2163228\ttotal: 5m 33s\tremaining: 7m 42s\n419:\tlearn: 0.2159903\ttotal: 5m 34s\tremaining: 7m 41s\n420:\tlearn: 0.2155804\ttotal: 5m 34s\tremaining: 7m 40s\n421:\tlearn: 0.2152142\ttotal: 5m 35s\tremaining: 7m 39s\n422:\tlearn: 0.2145805\ttotal: 5m 36s\tremaining: 7m 38s\n423:\tlearn: 0.2143077\ttotal: 5m 37s\tremaining: 7m 38s\n424:\tlearn: 0.2139553\ttotal: 5m 38s\tremaining: 7m 37s\n425:\tlearn: 0.2135163\ttotal: 5m 38s\tremaining: 7m 36s\n426:\tlearn: 0.2130529\ttotal: 5m 39s\tremaining: 7m 35s\n427:\tlearn: 0.2127716\ttotal: 5m 40s\tremaining: 7m 34s\n428:\tlearn: 0.2123845\ttotal: 5m 41s\tremaining: 7m 34s\n429:\tlearn: 0.2119481\ttotal: 5m 42s\tremaining: 7m 33s\n430:\tlearn: 0.2115576\ttotal: 5m 43s\tremaining: 7m 32s\n431:\tlearn: 0.2112707\ttotal: 5m 43s\tremaining: 7m 32s\n432:\tlearn: 0.2110392\ttotal: 5m 44s\tremaining: 7m 31s\n433:\tlearn: 0.2106070\ttotal: 5m 45s\tremaining: 7m 30s\n434:\tlearn: 0.2103361\ttotal: 5m 46s\tremaining: 7m 29s\n435:\tlearn: 0.2100504\ttotal: 5m 46s\tremaining: 7m 28s\n436:\tlearn: 0.2095873\ttotal: 5m 47s\tremaining: 7m 27s\n437:\tlearn: 0.2093052\ttotal: 5m 48s\tremaining: 7m 27s\n438:\tlearn: 0.2090004\ttotal: 5m 49s\tremaining: 7m 26s\n439:\tlearn: 0.2088024\ttotal: 5m 49s\tremaining: 7m 25s\n440:\tlearn: 0.2085531\ttotal: 5m 50s\tremaining: 7m 24s\n441:\tlearn: 0.2083408\ttotal: 5m 51s\tremaining: 7m 23s\n442:\tlearn: 0.2080337\ttotal: 5m 52s\tremaining: 7m 22s\n443:\tlearn: 0.2078187\ttotal: 5m 52s\tremaining: 7m 21s\n444:\tlearn: 0.2075311\ttotal: 5m 53s\tremaining: 7m 21s\n445:\tlearn: 0.2072738\ttotal: 5m 54s\tremaining: 7m 20s\n446:\tlearn: 0.2067522\ttotal: 5m 55s\tremaining: 7m 19s\n447:\tlearn: 0.2063481\ttotal: 5m 55s\tremaining: 7m 18s\n448:\tlearn: 0.2058386\ttotal: 5m 56s\tremaining: 7m 17s\n449:\tlearn: 0.2053626\ttotal: 5m 57s\tremaining: 7m 17s\n450:\tlearn: 0.2051054\ttotal: 5m 58s\tremaining: 7m 16s\n451:\tlearn: 0.2048575\ttotal: 5m 59s\tremaining: 7m 15s\n452:\tlearn: 0.2042536\ttotal: 5m 59s\tremaining: 7m 14s\n453:\tlearn: 0.2038463\ttotal: 6m\tremaining: 7m 13s\n454:\tlearn: 0.2034465\ttotal: 6m 1s\tremaining: 7m 12s\n455:\tlearn: 0.2032713\ttotal: 6m 2s\tremaining: 7m 12s\n456:\tlearn: 0.2029693\ttotal: 6m 2s\tremaining: 7m 11s\n457:\tlearn: 0.2027277\ttotal: 6m 3s\tremaining: 7m 10s\n458:\tlearn: 0.2024381\ttotal: 6m 4s\tremaining: 7m 9s\n459:\tlearn: 0.2020004\ttotal: 6m 5s\tremaining: 7m 8s\n460:\tlearn: 0.2014495\ttotal: 6m 5s\tremaining: 7m 7s\n461:\tlearn: 0.2010883\ttotal: 6m 6s\tremaining: 7m 7s\n462:\tlearn: 0.2008011\ttotal: 6m 7s\tremaining: 7m 6s\n463:\tlearn: 0.2005202\ttotal: 6m 8s\tremaining: 7m 5s\n464:\tlearn: 0.2002608\ttotal: 6m 8s\tremaining: 7m 4s\n465:\tlearn: 0.1998698\ttotal: 6m 9s\tremaining: 7m 3s\n466:\tlearn: 0.1996771\ttotal: 6m 10s\tremaining: 7m 2s\n467:\tlearn: 0.1993403\ttotal: 6m 11s\tremaining: 7m 2s\n468:\tlearn: 0.1989908\ttotal: 6m 11s\tremaining: 7m 1s\n469:\tlearn: 0.1985354\ttotal: 6m 12s\tremaining: 7m\n470:\tlearn: 0.1982856\ttotal: 6m 13s\tremaining: 6m 59s\n471:\tlearn: 0.1978625\ttotal: 6m 14s\tremaining: 6m 59s\n472:\tlearn: 0.1973801\ttotal: 6m 15s\tremaining: 6m 58s\n473:\tlearn: 0.1970981\ttotal: 6m 16s\tremaining: 6m 57s\n474:\tlearn: 0.1966941\ttotal: 6m 16s\tremaining: 6m 56s\n475:\tlearn: 0.1963512\ttotal: 6m 17s\tremaining: 6m 55s\n476:\tlearn: 0.1959638\ttotal: 6m 18s\tremaining: 6m 54s\n477:\tlearn: 0.1956597\ttotal: 6m 19s\tremaining: 6m 54s\n478:\tlearn: 0.1953698\ttotal: 6m 19s\tremaining: 6m 53s\n479:\tlearn: 0.1949223\ttotal: 6m 20s\tremaining: 6m 52s\n480:\tlearn: 0.1946718\ttotal: 6m 21s\tremaining: 6m 51s\n481:\tlearn: 0.1943968\ttotal: 6m 22s\tremaining: 6m 50s\n482:\tlearn: 0.1941778\ttotal: 6m 22s\tremaining: 6m 49s\n483:\tlearn: 0.1939136\ttotal: 6m 23s\tremaining: 6m 49s\n484:\tlearn: 0.1935819\ttotal: 6m 24s\tremaining: 6m 48s\n485:\tlearn: 0.1931615\ttotal: 6m 25s\tremaining: 6m 47s\n486:\tlearn: 0.1928688\ttotal: 6m 26s\tremaining: 6m 46s\n487:\tlearn: 0.1927372\ttotal: 6m 26s\tremaining: 6m 45s\n488:\tlearn: 0.1925008\ttotal: 6m 27s\tremaining: 6m 44s\n489:\tlearn: 0.1921221\ttotal: 6m 28s\tremaining: 6m 44s\n490:\tlearn: 0.1917547\ttotal: 6m 29s\tremaining: 6m 43s\n491:\tlearn: 0.1912412\ttotal: 6m 29s\tremaining: 6m 42s\n492:\tlearn: 0.1908016\ttotal: 6m 30s\tremaining: 6m 41s\n493:\tlearn: 0.1904498\ttotal: 6m 31s\tremaining: 6m 40s\n494:\tlearn: 0.1901099\ttotal: 6m 32s\tremaining: 6m 40s\n495:\tlearn: 0.1897942\ttotal: 6m 32s\tremaining: 6m 39s\n496:\tlearn: 0.1894387\ttotal: 6m 33s\tremaining: 6m 38s\n497:\tlearn: 0.1889589\ttotal: 6m 34s\tremaining: 6m 37s\n498:\tlearn: 0.1888032\ttotal: 6m 35s\tremaining: 6m 36s\n499:\tlearn: 0.1883031\ttotal: 6m 35s\tremaining: 6m 35s\n500:\tlearn: 0.1881030\ttotal: 6m 36s\tremaining: 6m 35s\n501:\tlearn: 0.1878723\ttotal: 6m 37s\tremaining: 6m 34s\n502:\tlearn: 0.1874673\ttotal: 6m 38s\tremaining: 6m 33s\n503:\tlearn: 0.1872012\ttotal: 6m 39s\tremaining: 6m 32s\n504:\tlearn: 0.1868741\ttotal: 6m 39s\tremaining: 6m 31s\n505:\tlearn: 0.1865414\ttotal: 6m 40s\tremaining: 6m 31s\n506:\tlearn: 0.1863356\ttotal: 6m 41s\tremaining: 6m 30s\n507:\tlearn: 0.1861895\ttotal: 6m 41s\tremaining: 6m 29s\n508:\tlearn: 0.1858732\ttotal: 6m 42s\tremaining: 6m 28s\n509:\tlearn: 0.1855485\ttotal: 6m 43s\tremaining: 6m 27s\n510:\tlearn: 0.1852062\ttotal: 6m 44s\tremaining: 6m 26s\n511:\tlearn: 0.1849390\ttotal: 6m 45s\tremaining: 6m 26s\n512:\tlearn: 0.1847692\ttotal: 6m 46s\tremaining: 6m 25s\n513:\tlearn: 0.1845128\ttotal: 6m 47s\tremaining: 6m 24s\n514:\tlearn: 0.1843149\ttotal: 6m 47s\tremaining: 6m 24s\n515:\tlearn: 0.1839829\ttotal: 6m 48s\tremaining: 6m 23s\n516:\tlearn: 0.1836593\ttotal: 6m 49s\tremaining: 6m 22s\n517:\tlearn: 0.1833232\ttotal: 6m 50s\tremaining: 6m 21s\n518:\tlearn: 0.1829710\ttotal: 6m 50s\tremaining: 6m 20s\n519:\tlearn: 0.1827035\ttotal: 6m 51s\tremaining: 6m 19s\n520:\tlearn: 0.1821881\ttotal: 6m 52s\tremaining: 6m 19s\n521:\tlearn: 0.1817461\ttotal: 6m 53s\tremaining: 6m 18s\n522:\tlearn: 0.1814363\ttotal: 6m 53s\tremaining: 6m 17s\n523:\tlearn: 0.1811940\ttotal: 6m 54s\tremaining: 6m 16s\n524:\tlearn: 0.1809212\ttotal: 6m 55s\tremaining: 6m 15s\n525:\tlearn: 0.1804248\ttotal: 6m 56s\tremaining: 6m 15s\n526:\tlearn: 0.1801277\ttotal: 6m 56s\tremaining: 6m 14s\n527:\tlearn: 0.1798191\ttotal: 6m 57s\tremaining: 6m 13s\n528:\tlearn: 0.1795330\ttotal: 6m 58s\tremaining: 6m 12s\n529:\tlearn: 0.1792215\ttotal: 6m 59s\tremaining: 6m 11s\n530:\tlearn: 0.1788037\ttotal: 6m 59s\tremaining: 6m 10s\n531:\tlearn: 0.1783997\ttotal: 7m\tremaining: 6m 10s\n532:\tlearn: 0.1781697\ttotal: 7m 1s\tremaining: 6m 9s\n533:\tlearn: 0.1779350\ttotal: 7m 2s\tremaining: 6m 8s\n534:\tlearn: 0.1777006\ttotal: 7m 3s\tremaining: 6m 7s\n535:\tlearn: 0.1774257\ttotal: 7m 3s\tremaining: 6m 6s\n536:\tlearn: 0.1771393\ttotal: 7m 4s\tremaining: 6m 6s\n537:\tlearn: 0.1769086\ttotal: 7m 5s\tremaining: 6m 5s\n538:\tlearn: 0.1766643\ttotal: 7m 6s\tremaining: 6m 4s\n539:\tlearn: 0.1763925\ttotal: 7m 6s\tremaining: 6m 3s\n540:\tlearn: 0.1760828\ttotal: 7m 7s\tremaining: 6m 2s\n541:\tlearn: 0.1758101\ttotal: 7m 8s\tremaining: 6m 1s\n542:\tlearn: 0.1755799\ttotal: 7m 9s\tremaining: 6m 1s\n543:\tlearn: 0.1753725\ttotal: 7m 9s\tremaining: 6m\n544:\tlearn: 0.1749704\ttotal: 7m 10s\tremaining: 5m 59s\n545:\tlearn: 0.1745385\ttotal: 7m 11s\tremaining: 5m 58s\n546:\tlearn: 0.1743680\ttotal: 7m 12s\tremaining: 5m 57s\n547:\tlearn: 0.1741253\ttotal: 7m 12s\tremaining: 5m 57s\n548:\tlearn: 0.1737229\ttotal: 7m 13s\tremaining: 5m 56s\n549:\tlearn: 0.1733616\ttotal: 7m 14s\tremaining: 5m 55s\n550:\tlearn: 0.1732108\ttotal: 7m 15s\tremaining: 5m 54s\n551:\tlearn: 0.1729729\ttotal: 7m 15s\tremaining: 5m 53s\n552:\tlearn: 0.1728173\ttotal: 7m 16s\tremaining: 5m 53s\n553:\tlearn: 0.1725550\ttotal: 7m 17s\tremaining: 5m 52s\n554:\tlearn: 0.1723255\ttotal: 7m 18s\tremaining: 5m 51s\n555:\tlearn: 0.1721040\ttotal: 7m 19s\tremaining: 5m 50s\n556:\tlearn: 0.1717726\ttotal: 7m 20s\tremaining: 5m 50s\n557:\tlearn: 0.1714480\ttotal: 7m 20s\tremaining: 5m 49s\n558:\tlearn: 0.1711630\ttotal: 7m 21s\tremaining: 5m 48s\n559:\tlearn: 0.1709380\ttotal: 7m 22s\tremaining: 5m 47s\n560:\tlearn: 0.1706264\ttotal: 7m 23s\tremaining: 5m 46s\n561:\tlearn: 0.1704263\ttotal: 7m 23s\tremaining: 5m 45s\n562:\tlearn: 0.1701636\ttotal: 7m 24s\tremaining: 5m 45s\n563:\tlearn: 0.1699352\ttotal: 7m 25s\tremaining: 5m 44s\n564:\tlearn: 0.1695692\ttotal: 7m 26s\tremaining: 5m 43s\n565:\tlearn: 0.1692972\ttotal: 7m 26s\tremaining: 5m 42s\n566:\tlearn: 0.1690021\ttotal: 7m 27s\tremaining: 5m 41s\n567:\tlearn: 0.1688883\ttotal: 7m 28s\tremaining: 5m 41s\n568:\tlearn: 0.1684722\ttotal: 7m 29s\tremaining: 5m 40s\n569:\tlearn: 0.1681723\ttotal: 7m 30s\tremaining: 5m 39s\n570:\tlearn: 0.1679719\ttotal: 7m 30s\tremaining: 5m 38s\n571:\tlearn: 0.1675988\ttotal: 7m 31s\tremaining: 5m 37s\n572:\tlearn: 0.1673237\ttotal: 7m 32s\tremaining: 5m 37s\n573:\tlearn: 0.1669551\ttotal: 7m 33s\tremaining: 5m 36s\n574:\tlearn: 0.1666772\ttotal: 7m 33s\tremaining: 5m 35s\n575:\tlearn: 0.1662971\ttotal: 7m 34s\tremaining: 5m 34s\n576:\tlearn: 0.1661784\ttotal: 7m 35s\tremaining: 5m 33s\n577:\tlearn: 0.1658615\ttotal: 7m 36s\tremaining: 5m 33s\n578:\tlearn: 0.1656472\ttotal: 7m 36s\tremaining: 5m 32s\n579:\tlearn: 0.1654938\ttotal: 7m 37s\tremaining: 5m 31s\n580:\tlearn: 0.1652623\ttotal: 7m 38s\tremaining: 5m 30s\n581:\tlearn: 0.1651003\ttotal: 7m 39s\tremaining: 5m 29s\n582:\tlearn: 0.1648165\ttotal: 7m 40s\tremaining: 5m 29s\n583:\tlearn: 0.1645652\ttotal: 7m 40s\tremaining: 5m 28s\n584:\tlearn: 0.1642410\ttotal: 7m 41s\tremaining: 5m 27s\n585:\tlearn: 0.1639533\ttotal: 7m 42s\tremaining: 5m 26s\n586:\tlearn: 0.1636390\ttotal: 7m 43s\tremaining: 5m 25s\n587:\tlearn: 0.1634874\ttotal: 7m 43s\tremaining: 5m 25s\n588:\tlearn: 0.1631057\ttotal: 7m 44s\tremaining: 5m 24s\n589:\tlearn: 0.1628091\ttotal: 7m 45s\tremaining: 5m 23s\n590:\tlearn: 0.1626720\ttotal: 7m 46s\tremaining: 5m 22s\n591:\tlearn: 0.1623354\ttotal: 7m 46s\tremaining: 5m 21s\n592:\tlearn: 0.1621086\ttotal: 7m 47s\tremaining: 5m 21s\n593:\tlearn: 0.1618438\ttotal: 7m 48s\tremaining: 5m 20s\n594:\tlearn: 0.1616103\ttotal: 7m 49s\tremaining: 5m 19s\n595:\tlearn: 0.1613153\ttotal: 7m 50s\tremaining: 5m 18s\n596:\tlearn: 0.1611418\ttotal: 7m 51s\tremaining: 5m 18s\n597:\tlearn: 0.1609504\ttotal: 7m 51s\tremaining: 5m 17s\n598:\tlearn: 0.1608238\ttotal: 7m 52s\tremaining: 5m 16s\n599:\tlearn: 0.1606992\ttotal: 7m 53s\tremaining: 5m 15s\n600:\tlearn: 0.1604765\ttotal: 7m 54s\tremaining: 5m 14s\n601:\tlearn: 0.1603445\ttotal: 7m 54s\tremaining: 5m 13s\n602:\tlearn: 0.1598541\ttotal: 7m 55s\tremaining: 5m 13s\n603:\tlearn: 0.1596850\ttotal: 7m 56s\tremaining: 5m 12s\n604:\tlearn: 0.1595196\ttotal: 7m 57s\tremaining: 5m 11s\n605:\tlearn: 0.1593363\ttotal: 7m 57s\tremaining: 5m 10s\n606:\tlearn: 0.1590778\ttotal: 7m 58s\tremaining: 5m 9s\n607:\tlearn: 0.1589393\ttotal: 7m 59s\tremaining: 5m 9s\n608:\tlearn: 0.1587328\ttotal: 8m\tremaining: 5m 8s\n609:\tlearn: 0.1585417\ttotal: 8m\tremaining: 5m 7s\n610:\tlearn: 0.1583812\ttotal: 8m 1s\tremaining: 5m 6s\n611:\tlearn: 0.1580858\ttotal: 8m 2s\tremaining: 5m 5s\n612:\tlearn: 0.1578677\ttotal: 8m 3s\tremaining: 5m 5s\n613:\tlearn: 0.1577012\ttotal: 8m 4s\tremaining: 5m 4s\n614:\tlearn: 0.1575737\ttotal: 8m 4s\tremaining: 5m 3s\n615:\tlearn: 0.1573591\ttotal: 8m 5s\tremaining: 5m 2s\n616:\tlearn: 0.1571762\ttotal: 8m 6s\tremaining: 5m 1s\n617:\tlearn: 0.1569756\ttotal: 8m 7s\tremaining: 5m 1s\n618:\tlearn: 0.1567156\ttotal: 8m 7s\tremaining: 5m\n619:\tlearn: 0.1565600\ttotal: 8m 8s\tremaining: 4m 59s\n620:\tlearn: 0.1563120\ttotal: 8m 9s\tremaining: 4m 58s\n621:\tlearn: 0.1561117\ttotal: 8m 10s\tremaining: 4m 57s\n622:\tlearn: 0.1558931\ttotal: 8m 10s\tremaining: 4m 57s\n623:\tlearn: 0.1556204\ttotal: 8m 11s\tremaining: 4m 56s\n624:\tlearn: 0.1554016\ttotal: 8m 12s\tremaining: 4m 55s\n625:\tlearn: 0.1551579\ttotal: 8m 13s\tremaining: 4m 54s\n626:\tlearn: 0.1550065\ttotal: 8m 13s\tremaining: 4m 53s\n627:\tlearn: 0.1547740\ttotal: 8m 14s\tremaining: 4m 53s\n628:\tlearn: 0.1545070\ttotal: 8m 15s\tremaining: 4m 52s\n629:\tlearn: 0.1543942\ttotal: 8m 16s\tremaining: 4m 51s\n630:\tlearn: 0.1540691\ttotal: 8m 16s\tremaining: 4m 50s\n631:\tlearn: 0.1538910\ttotal: 8m 17s\tremaining: 4m 49s\n632:\tlearn: 0.1536209\ttotal: 8m 18s\tremaining: 4m 48s\n633:\tlearn: 0.1532964\ttotal: 8m 19s\tremaining: 4m 48s\n634:\tlearn: 0.1531238\ttotal: 8m 19s\tremaining: 4m 47s\n635:\tlearn: 0.1529675\ttotal: 8m 20s\tremaining: 4m 46s\n636:\tlearn: 0.1527156\ttotal: 8m 21s\tremaining: 4m 45s\n637:\tlearn: 0.1524970\ttotal: 8m 22s\tremaining: 4m 45s\n638:\tlearn: 0.1522675\ttotal: 8m 23s\tremaining: 4m 44s\n639:\tlearn: 0.1518472\ttotal: 8m 24s\tremaining: 4m 43s\n640:\tlearn: 0.1515974\ttotal: 8m 24s\tremaining: 4m 42s\n641:\tlearn: 0.1513720\ttotal: 8m 25s\tremaining: 4m 41s\n642:\tlearn: 0.1511785\ttotal: 8m 26s\tremaining: 4m 41s\n643:\tlearn: 0.1508456\ttotal: 8m 27s\tremaining: 4m 40s\n644:\tlearn: 0.1506691\ttotal: 8m 27s\tremaining: 4m 39s\n645:\tlearn: 0.1503035\ttotal: 8m 28s\tremaining: 4m 38s\n646:\tlearn: 0.1500136\ttotal: 8m 29s\tremaining: 4m 37s\n647:\tlearn: 0.1498193\ttotal: 8m 30s\tremaining: 4m 37s\n648:\tlearn: 0.1496318\ttotal: 8m 31s\tremaining: 4m 36s\n649:\tlearn: 0.1492883\ttotal: 8m 31s\tremaining: 4m 35s\n650:\tlearn: 0.1490600\ttotal: 8m 32s\tremaining: 4m 34s\n651:\tlearn: 0.1489150\ttotal: 8m 33s\tremaining: 4m 33s\n652:\tlearn: 0.1485949\ttotal: 8m 34s\tremaining: 4m 33s\n653:\tlearn: 0.1483110\ttotal: 8m 34s\tremaining: 4m 32s\n654:\tlearn: 0.1481153\ttotal: 8m 35s\tremaining: 4m 31s\n655:\tlearn: 0.1479493\ttotal: 8m 36s\tremaining: 4m 30s\n656:\tlearn: 0.1476223\ttotal: 8m 37s\tremaining: 4m 30s\n657:\tlearn: 0.1473499\ttotal: 8m 37s\tremaining: 4m 29s\n658:\tlearn: 0.1471796\ttotal: 8m 38s\tremaining: 4m 28s\n659:\tlearn: 0.1470318\ttotal: 8m 39s\tremaining: 4m 27s\n660:\tlearn: 0.1467413\ttotal: 8m 40s\tremaining: 4m 26s\n661:\tlearn: 0.1466472\ttotal: 8m 41s\tremaining: 4m 26s\n662:\tlearn: 0.1464668\ttotal: 8m 41s\tremaining: 4m 25s\n663:\tlearn: 0.1463112\ttotal: 8m 42s\tremaining: 4m 24s\n664:\tlearn: 0.1460260\ttotal: 8m 43s\tremaining: 4m 23s\n665:\tlearn: 0.1457168\ttotal: 8m 44s\tremaining: 4m 22s\n666:\tlearn: 0.1453226\ttotal: 8m 44s\tremaining: 4m 22s\n667:\tlearn: 0.1451887\ttotal: 8m 45s\tremaining: 4m 21s\n668:\tlearn: 0.1450236\ttotal: 8m 46s\tremaining: 4m 20s\n669:\tlearn: 0.1448489\ttotal: 8m 47s\tremaining: 4m 19s\n670:\tlearn: 0.1446387\ttotal: 8m 47s\tremaining: 4m 18s\n671:\tlearn: 0.1444677\ttotal: 8m 48s\tremaining: 4m 18s\n672:\tlearn: 0.1441866\ttotal: 8m 49s\tremaining: 4m 17s\n673:\tlearn: 0.1439171\ttotal: 8m 50s\tremaining: 4m 16s\n674:\tlearn: 0.1437987\ttotal: 8m 50s\tremaining: 4m 15s\n675:\tlearn: 0.1435886\ttotal: 8m 51s\tremaining: 4m 14s\n676:\tlearn: 0.1434234\ttotal: 8m 52s\tremaining: 4m 14s\n677:\tlearn: 0.1432490\ttotal: 8m 53s\tremaining: 4m 13s\n678:\tlearn: 0.1431107\ttotal: 8m 54s\tremaining: 4m 12s\n679:\tlearn: 0.1428848\ttotal: 8m 55s\tremaining: 4m 11s\n680:\tlearn: 0.1426986\ttotal: 8m 55s\tremaining: 4m 10s\n681:\tlearn: 0.1424947\ttotal: 8m 56s\tremaining: 4m 10s\n682:\tlearn: 0.1423839\ttotal: 8m 57s\tremaining: 4m 9s\n683:\tlearn: 0.1420556\ttotal: 8m 58s\tremaining: 4m 8s\n684:\tlearn: 0.1419240\ttotal: 8m 58s\tremaining: 4m 7s\n685:\tlearn: 0.1416830\ttotal: 8m 59s\tremaining: 4m 7s\n686:\tlearn: 0.1415958\ttotal: 9m\tremaining: 4m 6s\n687:\tlearn: 0.1414191\ttotal: 9m 1s\tremaining: 4m 5s\n688:\tlearn: 0.1412676\ttotal: 9m 1s\tremaining: 4m 4s\n689:\tlearn: 0.1410825\ttotal: 9m 2s\tremaining: 4m 3s\n690:\tlearn: 0.1408600\ttotal: 9m 3s\tremaining: 4m 2s\n691:\tlearn: 0.1407224\ttotal: 9m 4s\tremaining: 4m 2s\n692:\tlearn: 0.1405276\ttotal: 9m 4s\tremaining: 4m 1s\n693:\tlearn: 0.1403644\ttotal: 9m 5s\tremaining: 4m\n694:\tlearn: 0.1401524\ttotal: 9m 6s\tremaining: 3m 59s\n695:\tlearn: 0.1399416\ttotal: 9m 7s\tremaining: 3m 59s\n696:\tlearn: 0.1398168\ttotal: 9m 7s\tremaining: 3m 58s\n697:\tlearn: 0.1395974\ttotal: 9m 8s\tremaining: 3m 57s\n698:\tlearn: 0.1393719\ttotal: 9m 9s\tremaining: 3m 56s\n699:\tlearn: 0.1392649\ttotal: 9m 10s\tremaining: 3m 55s\n700:\tlearn: 0.1390733\ttotal: 9m 10s\tremaining: 3m 54s\n701:\tlearn: 0.1388030\ttotal: 9m 11s\tremaining: 3m 54s\n702:\tlearn: 0.1385318\ttotal: 9m 12s\tremaining: 3m 53s\n703:\tlearn: 0.1383655\ttotal: 9m 13s\tremaining: 3m 52s\n704:\tlearn: 0.1382191\ttotal: 9m 13s\tremaining: 3m 51s\n705:\tlearn: 0.1380387\ttotal: 9m 14s\tremaining: 3m 51s\n706:\tlearn: 0.1378067\ttotal: 9m 15s\tremaining: 3m 50s\n707:\tlearn: 0.1375635\ttotal: 9m 16s\tremaining: 3m 49s\n708:\tlearn: 0.1373166\ttotal: 9m 17s\tremaining: 3m 48s\n709:\tlearn: 0.1371327\ttotal: 9m 17s\tremaining: 3m 47s\n710:\tlearn: 0.1369877\ttotal: 9m 18s\tremaining: 3m 47s\n711:\tlearn: 0.1367696\ttotal: 9m 19s\tremaining: 3m 46s\n712:\tlearn: 0.1365159\ttotal: 9m 20s\tremaining: 3m 45s\n713:\tlearn: 0.1363566\ttotal: 9m 20s\tremaining: 3m 44s\n714:\tlearn: 0.1361332\ttotal: 9m 21s\tremaining: 3m 43s\n715:\tlearn: 0.1358786\ttotal: 9m 22s\tremaining: 3m 43s\n716:\tlearn: 0.1355573\ttotal: 9m 23s\tremaining: 3m 42s\n717:\tlearn: 0.1354304\ttotal: 9m 23s\tremaining: 3m 41s\n718:\tlearn: 0.1352183\ttotal: 9m 24s\tremaining: 3m 40s\n719:\tlearn: 0.1349875\ttotal: 9m 25s\tremaining: 3m 40s\n720:\tlearn: 0.1346805\ttotal: 9m 26s\tremaining: 3m 39s\n721:\tlearn: 0.1346178\ttotal: 9m 27s\tremaining: 3m 38s\n722:\tlearn: 0.1344418\ttotal: 9m 28s\tremaining: 3m 37s\n723:\tlearn: 0.1342534\ttotal: 9m 28s\tremaining: 3m 36s\n724:\tlearn: 0.1340897\ttotal: 9m 29s\tremaining: 3m 36s\n725:\tlearn: 0.1339071\ttotal: 9m 30s\tremaining: 3m 35s\n726:\tlearn: 0.1336613\ttotal: 9m 31s\tremaining: 3m 34s\n727:\tlearn: 0.1335565\ttotal: 9m 31s\tremaining: 3m 33s\n728:\tlearn: 0.1333838\ttotal: 9m 32s\tremaining: 3m 32s\n729:\tlearn: 0.1331761\ttotal: 9m 33s\tremaining: 3m 32s\n730:\tlearn: 0.1328784\ttotal: 9m 34s\tremaining: 3m 31s\n731:\tlearn: 0.1326589\ttotal: 9m 34s\tremaining: 3m 30s\n732:\tlearn: 0.1324834\ttotal: 9m 35s\tremaining: 3m 29s\n733:\tlearn: 0.1321536\ttotal: 9m 36s\tremaining: 3m 28s\n734:\tlearn: 0.1319905\ttotal: 9m 37s\tremaining: 3m 28s\n735:\tlearn: 0.1317810\ttotal: 9m 38s\tremaining: 3m 27s\n736:\tlearn: 0.1314974\ttotal: 9m 38s\tremaining: 3m 26s\n737:\tlearn: 0.1313699\ttotal: 9m 39s\tremaining: 3m 25s\n738:\tlearn: 0.1311624\ttotal: 9m 40s\tremaining: 3m 24s\n739:\tlearn: 0.1310149\ttotal: 9m 41s\tremaining: 3m 24s\n740:\tlearn: 0.1307971\ttotal: 9m 41s\tremaining: 3m 23s\n741:\tlearn: 0.1306990\ttotal: 9m 42s\tremaining: 3m 22s\n742:\tlearn: 0.1305247\ttotal: 9m 43s\tremaining: 3m 21s\n743:\tlearn: 0.1304116\ttotal: 9m 44s\tremaining: 3m 20s\n744:\tlearn: 0.1301950\ttotal: 9m 44s\tremaining: 3m 20s\n745:\tlearn: 0.1301040\ttotal: 9m 45s\tremaining: 3m 19s\n746:\tlearn: 0.1299277\ttotal: 9m 46s\tremaining: 3m 18s\n747:\tlearn: 0.1296917\ttotal: 9m 47s\tremaining: 3m 17s\n748:\tlearn: 0.1295676\ttotal: 9m 47s\tremaining: 3m 17s\n749:\tlearn: 0.1292818\ttotal: 9m 48s\tremaining: 3m 16s\n750:\tlearn: 0.1290761\ttotal: 9m 49s\tremaining: 3m 15s\n751:\tlearn: 0.1289627\ttotal: 9m 50s\tremaining: 3m 14s\n752:\tlearn: 0.1287858\ttotal: 9m 51s\tremaining: 3m 13s\n753:\tlearn: 0.1285973\ttotal: 9m 51s\tremaining: 3m 13s\n754:\tlearn: 0.1285115\ttotal: 9m 52s\tremaining: 3m 12s\n755:\tlearn: 0.1283120\ttotal: 9m 53s\tremaining: 3m 11s\n756:\tlearn: 0.1282115\ttotal: 9m 54s\tremaining: 3m 10s\n757:\tlearn: 0.1280100\ttotal: 9m 54s\tremaining: 3m 9s\n758:\tlearn: 0.1278137\ttotal: 9m 55s\tremaining: 3m 9s\n759:\tlearn: 0.1276199\ttotal: 9m 56s\tremaining: 3m 8s\n760:\tlearn: 0.1275014\ttotal: 9m 57s\tremaining: 3m 7s\n761:\tlearn: 0.1273347\ttotal: 9m 58s\tremaining: 3m 6s\n762:\tlearn: 0.1272035\ttotal: 9m 59s\tremaining: 3m 6s\n763:\tlearn: 0.1270618\ttotal: 9m 59s\tremaining: 3m 5s\n764:\tlearn: 0.1269155\ttotal: 10m\tremaining: 3m 4s\n765:\tlearn: 0.1267338\ttotal: 10m 1s\tremaining: 3m 3s\n766:\tlearn: 0.1264959\ttotal: 10m 2s\tremaining: 3m 2s\n767:\tlearn: 0.1263363\ttotal: 10m 2s\tremaining: 3m 2s\n768:\tlearn: 0.1262376\ttotal: 10m 3s\tremaining: 3m 1s\n769:\tlearn: 0.1259682\ttotal: 10m 4s\tremaining: 3m\n770:\tlearn: 0.1257529\ttotal: 10m 5s\tremaining: 2m 59s\n771:\tlearn: 0.1256416\ttotal: 10m 5s\tremaining: 2m 58s\n772:\tlearn: 0.1254976\ttotal: 10m 6s\tremaining: 2m 58s\n773:\tlearn: 0.1254400\ttotal: 10m 7s\tremaining: 2m 57s\n774:\tlearn: 0.1253604\ttotal: 10m 8s\tremaining: 2m 56s\n775:\tlearn: 0.1252370\ttotal: 10m 9s\tremaining: 2m 55s\n776:\tlearn: 0.1250601\ttotal: 10m 9s\tremaining: 2m 55s\n777:\tlearn: 0.1248080\ttotal: 10m 10s\tremaining: 2m 54s\n778:\tlearn: 0.1245050\ttotal: 10m 11s\tremaining: 2m 53s\n779:\tlearn: 0.1243147\ttotal: 10m 12s\tremaining: 2m 52s\n780:\tlearn: 0.1242137\ttotal: 10m 12s\tremaining: 2m 51s\n781:\tlearn: 0.1239860\ttotal: 10m 13s\tremaining: 2m 51s\n782:\tlearn: 0.1239099\ttotal: 10m 14s\tremaining: 2m 50s\n783:\tlearn: 0.1237782\ttotal: 10m 15s\tremaining: 2m 49s\n784:\tlearn: 0.1236019\ttotal: 10m 15s\tremaining: 2m 48s\n785:\tlearn: 0.1234097\ttotal: 10m 16s\tremaining: 2m 47s\n786:\tlearn: 0.1232393\ttotal: 10m 17s\tremaining: 2m 47s\n787:\tlearn: 0.1230541\ttotal: 10m 18s\tremaining: 2m 46s\n788:\tlearn: 0.1229234\ttotal: 10m 19s\tremaining: 2m 45s\n789:\tlearn: 0.1227873\ttotal: 10m 19s\tremaining: 2m 44s\n790:\tlearn: 0.1226425\ttotal: 10m 20s\tremaining: 2m 43s\n791:\tlearn: 0.1224744\ttotal: 10m 21s\tremaining: 2m 43s\n792:\tlearn: 0.1222481\ttotal: 10m 22s\tremaining: 2m 42s\n793:\tlearn: 0.1220100\ttotal: 10m 22s\tremaining: 2m 41s\n794:\tlearn: 0.1218679\ttotal: 10m 23s\tremaining: 2m 40s\n795:\tlearn: 0.1216955\ttotal: 10m 24s\tremaining: 2m 40s\n796:\tlearn: 0.1215187\ttotal: 10m 25s\tremaining: 2m 39s\n797:\tlearn: 0.1213558\ttotal: 10m 26s\tremaining: 2m 38s\n798:\tlearn: 0.1211979\ttotal: 10m 26s\tremaining: 2m 37s\n799:\tlearn: 0.1210219\ttotal: 10m 27s\tremaining: 2m 36s\n800:\tlearn: 0.1208455\ttotal: 10m 28s\tremaining: 2m 36s\n801:\tlearn: 0.1207462\ttotal: 10m 29s\tremaining: 2m 35s\n802:\tlearn: 0.1205674\ttotal: 10m 30s\tremaining: 2m 34s\n803:\tlearn: 0.1204248\ttotal: 10m 31s\tremaining: 2m 33s\n804:\tlearn: 0.1203635\ttotal: 10m 31s\tremaining: 2m 33s\n805:\tlearn: 0.1201735\ttotal: 10m 32s\tremaining: 2m 32s\n806:\tlearn: 0.1200627\ttotal: 10m 33s\tremaining: 2m 31s\n807:\tlearn: 0.1198394\ttotal: 10m 34s\tremaining: 2m 30s\n808:\tlearn: 0.1196110\ttotal: 10m 34s\tremaining: 2m 29s\n809:\tlearn: 0.1194654\ttotal: 10m 35s\tremaining: 2m 29s\n810:\tlearn: 0.1192884\ttotal: 10m 36s\tremaining: 2m 28s\n811:\tlearn: 0.1191761\ttotal: 10m 37s\tremaining: 2m 27s\n812:\tlearn: 0.1190353\ttotal: 10m 37s\tremaining: 2m 26s\n813:\tlearn: 0.1188801\ttotal: 10m 38s\tremaining: 2m 25s\n814:\tlearn: 0.1187208\ttotal: 10m 39s\tremaining: 2m 25s\n815:\tlearn: 0.1185481\ttotal: 10m 40s\tremaining: 2m 24s\n816:\tlearn: 0.1184403\ttotal: 10m 40s\tremaining: 2m 23s\n817:\tlearn: 0.1183438\ttotal: 10m 41s\tremaining: 2m 22s\n818:\tlearn: 0.1182297\ttotal: 10m 42s\tremaining: 2m 21s\n819:\tlearn: 0.1180619\ttotal: 10m 43s\tremaining: 2m 21s\n820:\tlearn: 0.1178460\ttotal: 10m 43s\tremaining: 2m 20s\n821:\tlearn: 0.1176055\ttotal: 10m 44s\tremaining: 2m 19s\n822:\tlearn: 0.1174812\ttotal: 10m 45s\tremaining: 2m 18s\n823:\tlearn: 0.1172793\ttotal: 10m 46s\tremaining: 2m 18s\n824:\tlearn: 0.1171987\ttotal: 10m 47s\tremaining: 2m 17s\n825:\tlearn: 0.1169863\ttotal: 10m 47s\tremaining: 2m 16s\n826:\tlearn: 0.1168876\ttotal: 10m 48s\tremaining: 2m 15s\n827:\tlearn: 0.1167422\ttotal: 10m 49s\tremaining: 2m 14s\n828:\tlearn: 0.1165617\ttotal: 10m 50s\tremaining: 2m 14s\n829:\tlearn: 0.1163175\ttotal: 10m 50s\tremaining: 2m 13s\n830:\tlearn: 0.1161413\ttotal: 10m 51s\tremaining: 2m 12s\n831:\tlearn: 0.1159797\ttotal: 10m 52s\tremaining: 2m 11s\n832:\tlearn: 0.1158656\ttotal: 10m 53s\tremaining: 2m 10s\n833:\tlearn: 0.1157126\ttotal: 10m 53s\tremaining: 2m 10s\n834:\tlearn: 0.1155792\ttotal: 10m 54s\tremaining: 2m 9s\n835:\tlearn: 0.1154111\ttotal: 10m 55s\tremaining: 2m 8s\n836:\tlearn: 0.1152588\ttotal: 10m 56s\tremaining: 2m 7s\n837:\tlearn: 0.1151408\ttotal: 10m 56s\tremaining: 2m 6s\n838:\tlearn: 0.1149874\ttotal: 10m 57s\tremaining: 2m 6s\n839:\tlearn: 0.1148995\ttotal: 10m 58s\tremaining: 2m 5s\n840:\tlearn: 0.1148016\ttotal: 10m 59s\tremaining: 2m 4s\n841:\tlearn: 0.1146017\ttotal: 10m 59s\tremaining: 2m 3s\n842:\tlearn: 0.1144278\ttotal: 11m\tremaining: 2m 3s\n843:\tlearn: 0.1142171\ttotal: 11m 1s\tremaining: 2m 2s\n844:\tlearn: 0.1140961\ttotal: 11m 2s\tremaining: 2m 1s\n845:\tlearn: 0.1138583\ttotal: 11m 3s\tremaining: 2m\n846:\tlearn: 0.1137417\ttotal: 11m 4s\tremaining: 1m 59s\n847:\tlearn: 0.1135691\ttotal: 11m 4s\tremaining: 1m 59s\n848:\tlearn: 0.1134014\ttotal: 11m 5s\tremaining: 1m 58s\n849:\tlearn: 0.1133109\ttotal: 11m 6s\tremaining: 1m 57s\n850:\tlearn: 0.1131726\ttotal: 11m 7s\tremaining: 1m 56s\n851:\tlearn: 0.1130949\ttotal: 11m 7s\tremaining: 1m 56s\n852:\tlearn: 0.1128405\ttotal: 11m 8s\tremaining: 1m 55s\n853:\tlearn: 0.1127295\ttotal: 11m 9s\tremaining: 1m 54s\n854:\tlearn: 0.1125825\ttotal: 11m 10s\tremaining: 1m 53s\n855:\tlearn: 0.1124288\ttotal: 11m 10s\tremaining: 1m 52s\n856:\tlearn: 0.1122786\ttotal: 11m 11s\tremaining: 1m 52s\n857:\tlearn: 0.1120581\ttotal: 11m 12s\tremaining: 1m 51s\n858:\tlearn: 0.1119838\ttotal: 11m 13s\tremaining: 1m 50s\n859:\tlearn: 0.1118379\ttotal: 11m 13s\tremaining: 1m 49s\n860:\tlearn: 0.1117004\ttotal: 11m 14s\tremaining: 1m 48s\n861:\tlearn: 0.1115967\ttotal: 11m 15s\tremaining: 1m 48s\n862:\tlearn: 0.1115325\ttotal: 11m 16s\tremaining: 1m 47s\n863:\tlearn: 0.1112388\ttotal: 11m 17s\tremaining: 1m 46s\n864:\tlearn: 0.1110496\ttotal: 11m 17s\tremaining: 1m 45s\n865:\tlearn: 0.1109457\ttotal: 11m 18s\tremaining: 1m 45s\n866:\tlearn: 0.1107874\ttotal: 11m 19s\tremaining: 1m 44s\n867:\tlearn: 0.1106957\ttotal: 11m 20s\tremaining: 1m 43s\n868:\tlearn: 0.1105323\ttotal: 11m 20s\tremaining: 1m 42s\n869:\tlearn: 0.1104195\ttotal: 11m 21s\tremaining: 1m 41s\n870:\tlearn: 0.1103073\ttotal: 11m 22s\tremaining: 1m 41s\n871:\tlearn: 0.1102164\ttotal: 11m 23s\tremaining: 1m 40s\n872:\tlearn: 0.1101477\ttotal: 11m 23s\tremaining: 1m 39s\n873:\tlearn: 0.1100115\ttotal: 11m 24s\tremaining: 1m 38s\n874:\tlearn: 0.1098445\ttotal: 11m 25s\tremaining: 1m 37s\n875:\tlearn: 0.1096463\ttotal: 11m 26s\tremaining: 1m 37s\n876:\tlearn: 0.1094395\ttotal: 11m 27s\tremaining: 1m 36s\n877:\tlearn: 0.1092976\ttotal: 11m 27s\tremaining: 1m 35s\n878:\tlearn: 0.1092204\ttotal: 11m 28s\tremaining: 1m 34s\n879:\tlearn: 0.1091273\ttotal: 11m 29s\tremaining: 1m 33s\n880:\tlearn: 0.1089249\ttotal: 11m 30s\tremaining: 1m 33s\n881:\tlearn: 0.1087617\ttotal: 11m 30s\tremaining: 1m 32s\n882:\tlearn: 0.1085870\ttotal: 11m 31s\tremaining: 1m 31s\n883:\tlearn: 0.1083716\ttotal: 11m 32s\tremaining: 1m 30s\n884:\tlearn: 0.1082445\ttotal: 11m 33s\tremaining: 1m 30s\n885:\tlearn: 0.1081493\ttotal: 11m 34s\tremaining: 1m 29s\n886:\tlearn: 0.1080271\ttotal: 11m 35s\tremaining: 1m 28s\n887:\tlearn: 0.1078799\ttotal: 11m 35s\tremaining: 1m 27s\n888:\tlearn: 0.1077528\ttotal: 11m 36s\tremaining: 1m 26s\n889:\tlearn: 0.1075826\ttotal: 11m 37s\tremaining: 1m 26s\n890:\tlearn: 0.1073729\ttotal: 11m 38s\tremaining: 1m 25s\n891:\tlearn: 0.1072658\ttotal: 11m 38s\tremaining: 1m 24s\n892:\tlearn: 0.1071415\ttotal: 11m 39s\tremaining: 1m 23s\n893:\tlearn: 0.1070247\ttotal: 11m 40s\tremaining: 1m 23s\n894:\tlearn: 0.1068982\ttotal: 11m 41s\tremaining: 1m 22s\n895:\tlearn: 0.1068204\ttotal: 11m 41s\tremaining: 1m 21s\n896:\tlearn: 0.1066501\ttotal: 11m 42s\tremaining: 1m 20s\n897:\tlearn: 0.1064929\ttotal: 11m 43s\tremaining: 1m 19s\n898:\tlearn: 0.1063186\ttotal: 11m 44s\tremaining: 1m 19s\n899:\tlearn: 0.1061222\ttotal: 11m 45s\tremaining: 1m 18s\n900:\tlearn: 0.1060387\ttotal: 11m 45s\tremaining: 1m 17s\n901:\tlearn: 0.1059450\ttotal: 11m 46s\tremaining: 1m 16s\n902:\tlearn: 0.1058361\ttotal: 11m 47s\tremaining: 1m 15s\n903:\tlearn: 0.1057146\ttotal: 11m 48s\tremaining: 1m 15s\n904:\tlearn: 0.1055564\ttotal: 11m 48s\tremaining: 1m 14s\n905:\tlearn: 0.1053995\ttotal: 11m 49s\tremaining: 1m 13s\n906:\tlearn: 0.1051650\ttotal: 11m 50s\tremaining: 1m 12s\n907:\tlearn: 0.1050192\ttotal: 11m 51s\tremaining: 1m 12s\n908:\tlearn: 0.1049201\ttotal: 11m 52s\tremaining: 1m 11s\n909:\tlearn: 0.1047502\ttotal: 11m 52s\tremaining: 1m 10s\n910:\tlearn: 0.1046120\ttotal: 11m 53s\tremaining: 1m 9s\n911:\tlearn: 0.1044993\ttotal: 11m 54s\tremaining: 1m 8s\n912:\tlearn: 0.1043149\ttotal: 11m 55s\tremaining: 1m 8s\n913:\tlearn: 0.1040760\ttotal: 11m 55s\tremaining: 1m 7s\n914:\tlearn: 0.1039609\ttotal: 11m 56s\tremaining: 1m 6s\n915:\tlearn: 0.1038306\ttotal: 11m 57s\tremaining: 1m 5s\n916:\tlearn: 0.1037209\ttotal: 11m 58s\tremaining: 1m 5s\n917:\tlearn: 0.1036524\ttotal: 11m 58s\tremaining: 1m 4s\n918:\tlearn: 0.1035678\ttotal: 11m 59s\tremaining: 1m 3s\n919:\tlearn: 0.1034728\ttotal: 12m\tremaining: 1m 2s\n920:\tlearn: 0.1033478\ttotal: 12m 1s\tremaining: 1m 1s\n921:\tlearn: 0.1032449\ttotal: 12m 1s\tremaining: 1m 1s\n922:\tlearn: 0.1030900\ttotal: 12m 2s\tremaining: 1m\n923:\tlearn: 0.1030104\ttotal: 12m 3s\tremaining: 59.5s\n924:\tlearn: 0.1029346\ttotal: 12m 4s\tremaining: 58.7s\n925:\tlearn: 0.1028414\ttotal: 12m 5s\tremaining: 57.9s\n926:\tlearn: 0.1026780\ttotal: 12m 6s\tremaining: 57.2s\n927:\tlearn: 0.1025477\ttotal: 12m 6s\tremaining: 56.4s\n928:\tlearn: 0.1024127\ttotal: 12m 7s\tremaining: 55.6s\n929:\tlearn: 0.1022607\ttotal: 12m 8s\tremaining: 54.8s\n930:\tlearn: 0.1021848\ttotal: 12m 9s\tremaining: 54s\n931:\tlearn: 0.1020378\ttotal: 12m 9s\tremaining: 53.2s\n932:\tlearn: 0.1019251\ttotal: 12m 10s\tremaining: 52.5s\n933:\tlearn: 0.1018229\ttotal: 12m 11s\tremaining: 51.7s\n934:\tlearn: 0.1017025\ttotal: 12m 12s\tremaining: 50.9s\n935:\tlearn: 0.1015699\ttotal: 12m 12s\tremaining: 50.1s\n936:\tlearn: 0.1014637\ttotal: 12m 13s\tremaining: 49.3s\n937:\tlearn: 0.1012845\ttotal: 12m 14s\tremaining: 48.5s\n938:\tlearn: 0.1011505\ttotal: 12m 15s\tremaining: 47.8s\n939:\tlearn: 0.1009925\ttotal: 12m 15s\tremaining: 47s\n940:\tlearn: 0.1008159\ttotal: 12m 16s\tremaining: 46.2s\n941:\tlearn: 0.1007333\ttotal: 12m 17s\tremaining: 45.4s\n942:\tlearn: 0.1005806\ttotal: 12m 18s\tremaining: 44.6s\n943:\tlearn: 0.1003800\ttotal: 12m 18s\tremaining: 43.8s\n944:\tlearn: 0.1002775\ttotal: 12m 19s\tremaining: 43s\n945:\tlearn: 0.1001381\ttotal: 12m 20s\tremaining: 42.3s\n946:\tlearn: 0.0999896\ttotal: 12m 21s\tremaining: 41.5s\n947:\tlearn: 0.0999347\ttotal: 12m 21s\tremaining: 40.7s\n948:\tlearn: 0.0998719\ttotal: 12m 22s\tremaining: 39.9s\n949:\tlearn: 0.0997093\ttotal: 12m 23s\tremaining: 39.1s\n950:\tlearn: 0.0996369\ttotal: 12m 24s\tremaining: 38.3s\n951:\tlearn: 0.0995112\ttotal: 12m 24s\tremaining: 37.6s\n952:\tlearn: 0.0992718\ttotal: 12m 25s\tremaining: 36.8s\n953:\tlearn: 0.0992166\ttotal: 12m 26s\tremaining: 36s\n954:\tlearn: 0.0991424\ttotal: 12m 27s\tremaining: 35.2s\n955:\tlearn: 0.0990424\ttotal: 12m 28s\tremaining: 34.4s\n956:\tlearn: 0.0989277\ttotal: 12m 28s\tremaining: 33.6s\n957:\tlearn: 0.0988506\ttotal: 12m 29s\tremaining: 32.9s\n958:\tlearn: 0.0987024\ttotal: 12m 30s\tremaining: 32.1s\n959:\tlearn: 0.0986138\ttotal: 12m 31s\tremaining: 31.3s\n960:\tlearn: 0.0985541\ttotal: 12m 31s\tremaining: 30.5s\n961:\tlearn: 0.0983034\ttotal: 12m 32s\tremaining: 29.7s\n962:\tlearn: 0.0981795\ttotal: 12m 33s\tremaining: 28.9s\n963:\tlearn: 0.0981265\ttotal: 12m 34s\tremaining: 28.2s\n964:\tlearn: 0.0979025\ttotal: 12m 34s\tremaining: 27.4s\n965:\tlearn: 0.0978154\ttotal: 12m 35s\tremaining: 26.6s\n966:\tlearn: 0.0977145\ttotal: 12m 36s\tremaining: 25.8s\n967:\tlearn: 0.0975726\ttotal: 12m 37s\tremaining: 25s\n968:\tlearn: 0.0974792\ttotal: 12m 38s\tremaining: 24.3s\n969:\tlearn: 0.0973616\ttotal: 12m 38s\tremaining: 23.5s\n970:\tlearn: 0.0971458\ttotal: 12m 39s\tremaining: 22.7s\n971:\tlearn: 0.0970535\ttotal: 12m 40s\tremaining: 21.9s\n972:\tlearn: 0.0968855\ttotal: 12m 41s\tremaining: 21.1s\n973:\tlearn: 0.0967279\ttotal: 12m 42s\tremaining: 20.3s\n974:\tlearn: 0.0966001\ttotal: 12m 42s\tremaining: 19.6s\n975:\tlearn: 0.0964724\ttotal: 12m 43s\tremaining: 18.8s\n976:\tlearn: 0.0964040\ttotal: 12m 44s\tremaining: 18s\n977:\tlearn: 0.0962563\ttotal: 12m 45s\tremaining: 17.2s\n978:\tlearn: 0.0961779\ttotal: 12m 45s\tremaining: 16.4s\n979:\tlearn: 0.0960500\ttotal: 12m 46s\tremaining: 15.6s\n980:\tlearn: 0.0959593\ttotal: 12m 47s\tremaining: 14.9s\n981:\tlearn: 0.0958959\ttotal: 12m 48s\tremaining: 14.1s\n982:\tlearn: 0.0956683\ttotal: 12m 48s\tremaining: 13.3s\n983:\tlearn: 0.0955566\ttotal: 12m 49s\tremaining: 12.5s\n984:\tlearn: 0.0954619\ttotal: 12m 50s\tremaining: 11.7s\n985:\tlearn: 0.0953465\ttotal: 12m 51s\tremaining: 10.9s\n986:\tlearn: 0.0952555\ttotal: 12m 51s\tremaining: 10.2s\n987:\tlearn: 0.0951657\ttotal: 12m 52s\tremaining: 9.38s\n988:\tlearn: 0.0949430\ttotal: 12m 53s\tremaining: 8.6s\n989:\tlearn: 0.0948306\ttotal: 12m 54s\tremaining: 7.82s\n990:\tlearn: 0.0947536\ttotal: 12m 54s\tremaining: 7.04s\n991:\tlearn: 0.0946613\ttotal: 12m 55s\tremaining: 6.25s\n992:\tlearn: 0.0945508\ttotal: 12m 56s\tremaining: 5.47s\n993:\tlearn: 0.0944377\ttotal: 12m 57s\tremaining: 4.69s\n994:\tlearn: 0.0943614\ttotal: 12m 58s\tremaining: 3.91s\n995:\tlearn: 0.0942490\ttotal: 12m 58s\tremaining: 3.13s\n996:\tlearn: 0.0941788\ttotal: 12m 59s\tremaining: 2.35s\n997:\tlearn: 0.0940581\ttotal: 13m\tremaining: 1.56s\n998:\tlearn: 0.0938590\ttotal: 13m 1s\tremaining: 782ms\n999:\tlearn: 0.0937648\ttotal: 13m 1s\tremaining: 0us\n",
     "output_type": "stream"
    },
    {
     "execution_count": 28,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x79d5ed94bbe0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = np.squeeze(clf.predict(np.array(embeddings)))\n",
    "sum(predictions==np.array(labels))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:23:15.022663Z",
     "iopub.execute_input": "2024-04-23T05:23:15.023394Z",
     "iopub.status.idle": "2024-04-23T05:23:15.183785Z",
     "shell.execute_reply.started": "2024-04-23T05:23:15.023364Z",
     "shell.execute_reply": "2024-04-23T05:23:15.182863Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": [
    {
     "execution_count": 32,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9971469329529244"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "np.squeeze(clf.predict(np.array(embeddings)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:23:05.774823Z",
     "iopub.execute_input": "2024-04-23T05:23:05.775575Z",
     "iopub.status.idle": "2024-04-23T05:23:05.897390Z",
     "shell.execute_reply.started": "2024-04-23T05:23:05.775535Z",
     "shell.execute_reply": "2024-04-23T05:23:05.896322Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 31,
   "outputs": [
    {
     "execution_count": 31,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([5, 5, 5, ..., 5, 0, 1])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings_val = []\n",
    "labels_val = []\n",
    "\n",
    "for data, target in tqdm(val_loader_dino):\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings_val.extend(model(data).cpu().numpy())\n",
    "        \n",
    "    labels_val.extend(target)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:24:21.558576Z",
     "iopub.execute_input": "2024-04-23T05:24:21.559464Z",
     "iopub.status.idle": "2024-04-23T05:40:51.079239Z",
     "shell.execute_reply.started": "2024-04-23T05:24:21.559432Z",
     "shell.execute_reply": "2024-04-23T05:40:51.078247Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/47 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40e7fac5311843e39c27f2ed1a16385d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = np.squeeze(clf.predict(np.array(embeddings_val)))\n",
    "sum(predictions==np.array(labels_val))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T05:42:10.771287Z",
     "iopub.execute_input": "2024-04-23T05:42:10.771667Z",
     "iopub.status.idle": "2024-04-23T05:42:10.854113Z",
     "shell.execute_reply.started": "2024-04-23T05:42:10.771636Z",
     "shell.execute_reply": "2024-04-23T05:42:10.853103Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 34,
   "outputs": [
    {
     "execution_count": 34,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8126455906821963"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swinv2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('swinv2_base_window8_256.ms_in1k', pretrained=True)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:16:46.273501Z",
     "iopub.execute_input": "2024-04-23T18:16:46.274124Z",
     "iopub.status.idle": "2024-04-23T18:16:55.591281Z",
     "shell.execute_reply.started": "2024-04-23T18:16:46.274091Z",
     "shell.execute_reply": "2024-04-23T18:16:55.590309Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b438b511b7f04d63826fe0e9928767c4"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "transforms_256 = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(size=(256, 256)),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "trainset_256 = ClassificationDataset(metadata.iloc[train_idx][['path', 'dx_encoded']],\n",
    "                                 transform=transforms_256)\n",
    "valset_256 = ClassificationDataset(metadata.iloc[valid_idx][['path', 'dx_encoded']],\n",
    "                                 transform=transforms_256)\n",
    "train_loader_256 = torch.utils.data.DataLoader(trainset_256, batch_size=64,\n",
    "                                                      shuffle=False,\n",
    "                                           collate_fn = collate_fn)\n",
    "val_loader_256 = torch.utils.data.DataLoader(valset_256, batch_size=64,\n",
    "                                         shuffle=False, collate_fn = collate_fn)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:38:20.602679Z",
     "iopub.execute_input": "2024-04-23T18:38:20.603064Z",
     "iopub.status.idle": "2024-04-23T18:38:20.619817Z",
     "shell.execute_reply.started": "2024-04-23T18:38:20.603036Z",
     "shell.execute_reply": "2024-04-23T18:38:20.618782Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for data, target in tqdm(train_loader_256):\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings.extend(model(data).cpu().numpy())\n",
    "        \n",
    "    labels.extend(target)\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:38:22.183139Z",
     "iopub.execute_input": "2024-04-23T18:38:22.183488Z",
     "iopub.status.idle": "2024-04-23T18:42:59.544368Z",
     "shell.execute_reply.started": "2024-04-23T18:38:22.183459Z",
     "shell.execute_reply": "2024-04-23T18:42:59.543426Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8944513074542519bb552c08e790030"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "\n",
    "clf.fit(np.array(embeddings), labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:54:48.722585Z",
     "iopub.execute_input": "2024-04-23T18:54:48.723412Z",
     "iopub.status.idle": "2024-04-23T18:55:07.398218Z",
     "shell.execute_reply.started": "2024-04-23T18:54:48.723377Z",
     "shell.execute_reply": "2024-04-23T18:55:07.397177Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "execution_count": 21,
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = clf.predict(np.array(embeddings))\n",
    "sum(predictions==np.array(labels))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:55:12.010565Z",
     "iopub.execute_input": "2024-04-23T18:55:12.011314Z",
     "iopub.status.idle": "2024-04-23T18:55:32.155679Z",
     "shell.execute_reply.started": "2024-04-23T18:55:12.011280Z",
     "shell.execute_reply": "2024-04-23T18:55:32.154572Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "execution_count": 22,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8114122681883025"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings_val = []\n",
    "labels_val = []\n",
    "\n",
    "for data, target in tqdm(val_loader_256):\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings_val.extend(model(data).cpu().numpy())\n",
    "        \n",
    "    labels_val.extend(target)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T18:57:07.759197Z",
     "iopub.execute_input": "2024-04-23T18:57:07.759893Z",
     "iopub.status.idle": "2024-04-23T18:59:06.058720Z",
     "shell.execute_reply.started": "2024-04-23T18:57:07.759859Z",
     "shell.execute_reply": "2024-04-23T18:59:06.057716Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/47 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5be8de2a5354c60971482c52b093cb8"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = np.squeeze(clf.predict(np.array(embeddings_val)))\n",
    "sum(predictions==np.array(labels_val))/predictions.shape[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T19:00:28.238975Z",
     "iopub.execute_input": "2024-04-23T19:00:28.239773Z",
     "iopub.status.idle": "2024-04-23T19:00:38.087855Z",
     "shell.execute_reply.started": "2024-04-23T19:00:28.239732Z",
     "shell.execute_reply": "2024-04-23T19:00:38.086899Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.762063227953411"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base dinov2 + linear layer and peft training"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_classes = 7\n",
    "\n",
    "num_features = 768\n",
    "\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(num_features, num_classes)\n",
    ")\n",
    "model=model.to(device)    \n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T19:38:35.326942Z",
     "iopub.execute_input": "2024-04-23T19:38:35.327631Z",
     "iopub.status.idle": "2024-04-23T19:38:37.111689Z",
     "shell.execute_reply.started": "2024-04-23T19:38:35.327597Z",
     "shell.execute_reply": "2024-04-23T19:38:37.110723Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": "cls_token False\npos_embed False\npatch_embed.proj.weight False\npatch_embed.proj.bias False\nblocks.0.norm1.weight False\nblocks.0.norm1.bias False\nblocks.0.attn.qkv.weight False\nblocks.0.attn.qkv.bias False\nblocks.0.attn.proj.weight False\nblocks.0.attn.proj.bias False\nblocks.0.ls1.gamma False\nblocks.0.norm2.weight False\nblocks.0.norm2.bias False\nblocks.0.mlp.fc1.weight False\nblocks.0.mlp.fc1.bias False\nblocks.0.mlp.fc2.weight False\nblocks.0.mlp.fc2.bias False\nblocks.0.ls2.gamma False\nblocks.1.norm1.weight False\nblocks.1.norm1.bias False\nblocks.1.attn.qkv.weight False\nblocks.1.attn.qkv.bias False\nblocks.1.attn.proj.weight False\nblocks.1.attn.proj.bias False\nblocks.1.ls1.gamma False\nblocks.1.norm2.weight False\nblocks.1.norm2.bias False\nblocks.1.mlp.fc1.weight False\nblocks.1.mlp.fc1.bias False\nblocks.1.mlp.fc2.weight False\nblocks.1.mlp.fc2.bias False\nblocks.1.ls2.gamma False\nblocks.2.norm1.weight False\nblocks.2.norm1.bias False\nblocks.2.attn.qkv.weight False\nblocks.2.attn.qkv.bias False\nblocks.2.attn.proj.weight False\nblocks.2.attn.proj.bias False\nblocks.2.ls1.gamma False\nblocks.2.norm2.weight False\nblocks.2.norm2.bias False\nblocks.2.mlp.fc1.weight False\nblocks.2.mlp.fc1.bias False\nblocks.2.mlp.fc2.weight False\nblocks.2.mlp.fc2.bias False\nblocks.2.ls2.gamma False\nblocks.3.norm1.weight False\nblocks.3.norm1.bias False\nblocks.3.attn.qkv.weight False\nblocks.3.attn.qkv.bias False\nblocks.3.attn.proj.weight False\nblocks.3.attn.proj.bias False\nblocks.3.ls1.gamma False\nblocks.3.norm2.weight False\nblocks.3.norm2.bias False\nblocks.3.mlp.fc1.weight False\nblocks.3.mlp.fc1.bias False\nblocks.3.mlp.fc2.weight False\nblocks.3.mlp.fc2.bias False\nblocks.3.ls2.gamma False\nblocks.4.norm1.weight False\nblocks.4.norm1.bias False\nblocks.4.attn.qkv.weight False\nblocks.4.attn.qkv.bias False\nblocks.4.attn.proj.weight False\nblocks.4.attn.proj.bias False\nblocks.4.ls1.gamma False\nblocks.4.norm2.weight False\nblocks.4.norm2.bias False\nblocks.4.mlp.fc1.weight False\nblocks.4.mlp.fc1.bias False\nblocks.4.mlp.fc2.weight False\nblocks.4.mlp.fc2.bias False\nblocks.4.ls2.gamma False\nblocks.5.norm1.weight False\nblocks.5.norm1.bias False\nblocks.5.attn.qkv.weight False\nblocks.5.attn.qkv.bias False\nblocks.5.attn.proj.weight False\nblocks.5.attn.proj.bias False\nblocks.5.ls1.gamma False\nblocks.5.norm2.weight False\nblocks.5.norm2.bias False\nblocks.5.mlp.fc1.weight False\nblocks.5.mlp.fc1.bias False\nblocks.5.mlp.fc2.weight False\nblocks.5.mlp.fc2.bias False\nblocks.5.ls2.gamma False\nblocks.6.norm1.weight False\nblocks.6.norm1.bias False\nblocks.6.attn.qkv.weight False\nblocks.6.attn.qkv.bias False\nblocks.6.attn.proj.weight False\nblocks.6.attn.proj.bias False\nblocks.6.ls1.gamma False\nblocks.6.norm2.weight False\nblocks.6.norm2.bias False\nblocks.6.mlp.fc1.weight False\nblocks.6.mlp.fc1.bias False\nblocks.6.mlp.fc2.weight False\nblocks.6.mlp.fc2.bias False\nblocks.6.ls2.gamma False\nblocks.7.norm1.weight False\nblocks.7.norm1.bias False\nblocks.7.attn.qkv.weight False\nblocks.7.attn.qkv.bias False\nblocks.7.attn.proj.weight False\nblocks.7.attn.proj.bias False\nblocks.7.ls1.gamma False\nblocks.7.norm2.weight False\nblocks.7.norm2.bias False\nblocks.7.mlp.fc1.weight False\nblocks.7.mlp.fc1.bias False\nblocks.7.mlp.fc2.weight False\nblocks.7.mlp.fc2.bias False\nblocks.7.ls2.gamma False\nblocks.8.norm1.weight False\nblocks.8.norm1.bias False\nblocks.8.attn.qkv.weight False\nblocks.8.attn.qkv.bias False\nblocks.8.attn.proj.weight False\nblocks.8.attn.proj.bias False\nblocks.8.ls1.gamma False\nblocks.8.norm2.weight False\nblocks.8.norm2.bias False\nblocks.8.mlp.fc1.weight False\nblocks.8.mlp.fc1.bias False\nblocks.8.mlp.fc2.weight False\nblocks.8.mlp.fc2.bias False\nblocks.8.ls2.gamma False\nblocks.9.norm1.weight False\nblocks.9.norm1.bias False\nblocks.9.attn.qkv.weight False\nblocks.9.attn.qkv.bias False\nblocks.9.attn.proj.weight False\nblocks.9.attn.proj.bias False\nblocks.9.ls1.gamma False\nblocks.9.norm2.weight False\nblocks.9.norm2.bias False\nblocks.9.mlp.fc1.weight False\nblocks.9.mlp.fc1.bias False\nblocks.9.mlp.fc2.weight False\nblocks.9.mlp.fc2.bias False\nblocks.9.ls2.gamma False\nblocks.10.norm1.weight False\nblocks.10.norm1.bias False\nblocks.10.attn.qkv.weight False\nblocks.10.attn.qkv.bias False\nblocks.10.attn.proj.weight False\nblocks.10.attn.proj.bias False\nblocks.10.ls1.gamma False\nblocks.10.norm2.weight False\nblocks.10.norm2.bias False\nblocks.10.mlp.fc1.weight False\nblocks.10.mlp.fc1.bias False\nblocks.10.mlp.fc2.weight False\nblocks.10.mlp.fc2.bias False\nblocks.10.ls2.gamma False\nblocks.11.norm1.weight False\nblocks.11.norm1.bias False\nblocks.11.attn.qkv.weight False\nblocks.11.attn.qkv.bias False\nblocks.11.attn.proj.weight False\nblocks.11.attn.proj.bias False\nblocks.11.ls1.gamma False\nblocks.11.norm2.weight False\nblocks.11.norm2.bias False\nblocks.11.mlp.fc1.weight False\nblocks.11.mlp.fc1.bias False\nblocks.11.mlp.fc2.weight False\nblocks.11.mlp.fc2.bias False\nblocks.11.ls2.gamma False\nnorm.weight False\nnorm.bias False\nhead.0.weight True\nhead.0.bias True\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"head.0\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model=model.to(device)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T19:52:21.787738Z",
     "iopub.execute_input": "2024-04-23T19:52:21.788129Z",
     "iopub.status.idle": "2024-04-23T19:52:21.805965Z",
     "shell.execute_reply.started": "2024-04-23T19:52:21.788099Z",
     "shell.execute_reply": "2024-04-23T19:52:21.804952Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable params: 12,400 || all params: 86,597,495 || trainable%: 0.014319120893739478\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, optimizer,\n",
    "                                                                 10, train_loader, val_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T19:52:37.334038Z",
     "iopub.execute_input": "2024-04-23T19:52:37.334907Z",
     "iopub.status.idle": "2024-04-23T22:46:11.318992Z",
     "shell.execute_reply.started": "2024-04-23T19:52:37.334863Z",
     "shell.execute_reply": "2024-04-23T22:46:11.317730Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 0\n train loss: 0.6071464641527696, train acc: 0.7732954545454546\n val loss: 0.6786566928346106, val acc: 0.7531869037354246\n\nEpoch 1\n train loss: 0.5782031683081931, train acc: 0.7869318181818182\n val loss: 0.6222907472798165, val acc: 0.7734661590545735\n\nEpoch 2\n train loss: 0.5487538021756336, train acc: 0.7940340909090909\n val loss: 0.6230489059965661, val acc: 0.7677801726980412\n\nEpoch 3\n train loss: 0.5157719948075035, train acc: 0.8063920454545455\n val loss: 0.5665523105479301, val acc: 0.791418286714148\n\nEpoch 4\n train loss: 0.49756884944032537, train acc: 0.8142045454545455\n val loss: 0.5741238365782068, val acc: 0.7847349599320838\n\nEpoch 5\n train loss: 0.4863752846013416, train acc: 0.81875\n val loss: 0.5801024725462528, val acc: 0.778086023761871\n\nEpoch 6\n train loss: 0.4727133125574751, train acc: 0.8247159090909091\n val loss: 0.5689197141439357, val acc: 0.7910514492937859\n\nEpoch 7\n train loss: 0.46172675061970947, train acc: 0.8276988636363637\n val loss: 0.5651326715312106, val acc: 0.7853654623031616\n\nEpoch 8\n train loss: 0.4511096778241071, train acc: 0.8278409090909091\n val loss: 0.5755095382003074, val acc: 0.7954076484162756\n\nEpoch 9\n train loss: 0.4492224091748622, train acc: 0.8306818181818182\n val loss: 0.5577062948587093, val acc: 0.7960381514214455\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}