{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8274525,"sourceType":"datasetVersion","datasetId":4913304}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T21:04:22.672147Z","iopub.execute_input":"2024-05-01T21:04:22.672603Z","iopub.status.idle":"2024-05-01T21:04:22.708560Z","shell.execute_reply.started":"2024-05-01T21:04:22.672567Z","shell.execute_reply":"2024-05-01T21:04:22.707636Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:04:22.710465Z","iopub.execute_input":"2024-05-01T21:04:22.711234Z","iopub.status.idle":"2024-05-01T21:04:39.480685Z","shell.execute_reply.started":"2024-05-01T21:04:22.711195Z","shell.execute_reply":"2024-05-01T21:04:39.479348Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m575.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from einops import rearrange, repeat,reduce\nfrom einops.layers.torch import Rearrange\nfrom torch import linalg as LA","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:04:55.144739Z","iopub.execute_input":"2024-05-01T21:04:55.145156Z","iopub.status.idle":"2024-05-01T21:05:01.413130Z","shell.execute_reply.started":"2024-05-01T21:04:55.145123Z","shell.execute_reply":"2024-05-01T21:05:01.411901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport time\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom torch import linalg as LA\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nloc_time = time.strftime(\"%H%M%S\", time.localtime()) \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nratio = 8\n# c_outs= 128\nreduction = 1 # default:1\nn_iter = 0\n\ndef squash(inputs,ep_iter=n_iter,total=100):\n    beta = 1.45 - 0.22* (ep_iter/total)\n    mag_sq = torch.sum(inputs**2, dim=2, keepdim=True)\n    mag = torch.sqrt(mag_sq)\n    s = (mag_sq / (beta + mag_sq)) * (inputs / mag)\n    return s\n\n\nclass CapsNet(nn.Module):\n    def __init__(self,conv_inputs,\n                 num_classes=7,\n                 init_weights=False,\n                 conv_outputs = 128,\n                 primary_units = 8,#8,\n                 primary_unit_size = 576,# 16 * 6 * 6,\n                 output_unit_size = 16,):\n        super().__init__()\n        \n        self.Convolution = nn.Sequential(nn.Conv2d(conv_inputs, conv_outputs, 21,stride=2),\n                                        nn.BatchNorm2d(conv_outputs),\n                                        nn.ReLU(inplace=True),)\n\n        \n\n        # self.Pool = nn.FractionalMaxPool2d(3, output_size=(20))\n        self.Pool = nn.AdaptiveMaxPool2d(20)\n        #Attention\n        self.CBAM = Conv_CBAM(conv_outputs,conv_outputs)\n        #Capsule\n        self.primary = Primary_Caps(in_channels=conv_outputs,#128\n                                    caps_units=primary_units,#8\n                                    )\n\n        self.digits = Digits_Caps(in_units=primary_units,#8\n                                   in_channels=primary_unit_size,#16*6*6=576\n                                   num_units=num_classes,#classification_num\n                                   unit_size=output_unit_size,#16\n                                   )\n        if init_weights:\n            self._initialize_weights()\n        \n    def forward(self, x):\n        x = self.Convolution(x)\n        x = self.Pool(x)      \n        x = self.CBAM(x)\n        out = self.digits(self.primary(x))\n        return out\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):# or isinstance(m, nn.BatchNorm2d):\n                nn.init.normal_(m.weight, 0, 0.01)\n                # nn.init.constant_(m.bias, 0)\n    #margin_loss           \n    def loss(self,img_input, target, epoch=1,epoch_total=100, size_average=True):\n    # def loss(self,img_input, target, size_average=True):\n        batch_size = img_input.size(0)\n        # coefficient = epoch/epoch_total\n        \n        v_mag = LA.norm(img_input,ord=2,dim=(2,3),keepdim=True) #largest singular value\n        zero = Variable(torch.zeros(1)).to(device)\n        m_plus  = 0.9 #- (coefficient/10) \n        m_minus =0.1 #+ (coefficient/10)\n        max_l = torch.max(m_plus - v_mag, zero).view(batch_size, -1)**2\n        max_r = torch.max(v_mag - m_minus, zero).view(batch_size, -1)**2\n        \n        # init_lambda = 0.5 \n        loss_lambda = 0.5 #+ coefficient\n        # print(f\"lambda:{loss_lambda}\")\n        T_c = target\n        L_c = T_c * max_l + loss_lambda * (1.0 - T_c) * max_r\n        L_c = torch.sum(L_c,1)\n        \n        if size_average:\n            L_c = torch.mean(L_c)\n\n        return L_c\n\n    def update_n_iter(self, ep_iter):\n        if ep_iter > 100 and ep_iter % 10 == 0:\n            ep_iter -=100\n            self.primary.n_iter = ep_iter\n            self.digits.n_iter = ep_iter\n            beta = 1.45 - 0.22* (ep_iter/100)\n            print(f\"beta:{beta}\")\n        \n        \nclass Primary_Caps(nn.Module):\n    def __init__(self, in_channels, caps_units):\n        super(Primary_Caps, self).__init__()\n        self.n_iter = n_iter\n        self.in_channels = in_channels\n        self.caps_units = caps_units\n        \n        def create_conv_unit(unit_idx):\n            unit = ConvUnit(in_channels=in_channels)\n            self.add_module(\"Caps_\" + str(unit_idx), unit)\n            return unit\n        self.units = [create_conv_unit(i) for i in range(self.caps_units)]\n   \n    #no_routing\n    def forward(self, x):\n        # Get output for each unit.\n        # Each will be (batch, channels, height, width).\n        u = [self.units[i](x) for i in range(self.caps_units)]\n        # Stack all unit outputs (batch, unit, channels, height, width).\n        u = torch.stack(u, dim=1)\n        # Flatten to (batch, unit, output).\n        u = u.view(x.size(0), self.caps_units, -1)\n\n        return squash(u,self.n_iter)\n    \nclass Digits_Caps(nn.Module):\n    def __init__(self, in_units, in_channels, num_units, unit_size):\n        super(Digits_Caps, self).__init__()\n        self.n_iter = n_iter\n        self.in_units = in_units\n        self.in_channels = in_channels\n        self.num_units = num_units\n        \n        self.W = nn.Parameter(torch.randn(1, in_channels, self.num_units, unit_size, in_units))\n        # self.w = [1,576,7,16,8]\n        \n    #routing\n    def forward(self, x):\n        batch_size = x.size(0)    \n        # (batch, in_units, features) -> (batch, features, in_units)\n        x = x.transpose(1, 2)        \n        # (batch, features, in_units) -> (batch, features, num_units, in_units, 1)\n        x = torch.stack([x] * self.num_units, dim=2).unsqueeze(4)        \n        # (batch, features, in_units, unit_size, num_units)\n        W = torch.cat([self.W] * batch_size, dim=0)\n        # Transform inputs by weight matrix.\n        # (batch_size, features, num_units, unit_size, 1)\n        u_hat = torch.matmul(W, x)\n        # Initialize routing logits to zero.\n        b_ij = Variable(torch.zeros(1, self.in_channels, self.num_units, 1)).to(device)\n        \n        num_iterations = 3\n        for iteration in range(num_iterations):\n            # Convert routing logits to softmax.\n            # (batch, features, num_units, 1, 1)\n            #c_ij = F.softmax(b_ij, dim=0)\n            c_ij = b_ij.softmax(dim=1)\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n\n            # Apply routing (c_ij) to weighted inputs (u_hat).\n            # (batch_size, 1, num_units, unit_size, 1)\n            s_j = torch.sum(c_ij * u_hat, dim=1, keepdim=True)\n\n            # (batch_size, 1, num_units, unit_size, 1)\n            v_j = squash(s_j,self.n_iter)\n\n            # (batch_size, features, num_units, unit_size, 1)\n            v_j1 = torch.cat([v_j] * self.in_channels, dim=1)\n\n            # (batch_size, features, num_units, 1)\n            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n\n            # Update b_ij (routing)\n            b_ij = u_vj1\n\n        # return v_j.squeeze(1)\n        return rearrange(v_j.squeeze(1), 'b c (g h) w -> b c g (h w)',g=4)\n                \nclass ConvUnit(nn.Module):\n    def __init__(self, in_channels):\n        super(ConvUnit, self).__init__()\n        Caps_out = in_channels // ratio# 16\n        self.Cpas = nn.Sequential(\n                        # nn.Conv2d(in_channels,Caps_out,(9,9),stride=2,groups=Caps_out, bias=False),\n                        nn.Conv2d(in_channels,in_channels,(9,1),stride=1, bias=False),\n                        nn.Conv2d(in_channels,Caps_out,(1,9),stride=2,groups=Caps_out, bias=False),\n                    )\n\n    def forward(self, x):\n        output = self.Cpas(x)\n        return output\n\nclass Conv_CBAM(nn.Module):\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n        super(Conv_CBAM, self).__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.Hardswish() if act else nn.Identity()\n        self.ca = ChannelAttention(c2, reduction=reduction)\n        self.sa = SpatialAttention()\n\n    def forward(self, x):\n        x = self.act(self.bn(self.conv(x)))\n        x = self.ca(x) * x\n        x = self.sa(x) * x\n        return x\n    \ndef autopad(k, p=None):  # kernel, padding\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n    return p\n\n# SAM\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=3): # default:3\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7)\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size,padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # This is different from the paper[S. Woo, et al. \"CBAM: Convolutional Block Attention Module,\"].\n        avg_out = torch.mean(x, dim=1, keepdim=True)#The different channels are averaged and converted to 1 channel.\n        max_out, _ = torch.max(x, dim=1, keepdim=True)#Maximizing the different channels and making them 1-channel.\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n# CAM\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(ChannelAttention, self).__init__()\n        me_c = channels // reduction\n        self.avg_pool = nn.AdaptiveAvgPool2d(1) \n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1   = nn.Conv2d(channels, me_c, 1, bias=False)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.fc2   = nn.Conv2d(me_c, channels, 1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:02.046908Z","iopub.execute_input":"2024-05-01T21:05:02.047443Z","iopub.status.idle":"2024-05-01T21:05:02.199891Z","shell.execute_reply.started":"2024-05-01T21:05:02.047410Z","shell.execute_reply":"2024-05-01T21:05:02.198617Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install thop","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:05.062814Z","iopub.execute_input":"2024-05-01T21:05:05.063226Z","iopub.status.idle":"2024-05-01T21:05:20.166887Z","shell.execute_reply.started":"2024-05-01T21:05:05.063197Z","shell.execute_reply":"2024-05-01T21:05:20.165532Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport seaborn as sns\nimport prettytable\nimport matplotlib.pyplot as plt\nfrom thop.profile import profile\nimport time, random\n\noffset = 16 #draw_size_acc\nclass  ImageShow(object):\n    def __init__(self,\n                train_loss_list,train_acc_list,\n                test_loss_list,test_acc_list,\n                test_auc_list,\n                val_loss_list,val_acc_list):        \n        self.trainll, self.trainacl = train_loss_list, train_acc_list\n        self.testll, self.testacl = test_loss_list, test_acc_list\n        self.testauc = test_auc_list\n        self.valll, self.valacl = val_loss_list, val_acc_list\n        \n    def train(self,opt=\"Loss\",write=True,custom_path=None,img_title=None,suf=None):\n        if opt == 'Acc':\n            img_portray(opt=opt,write=write,dates=self.trainacl,\n                        #location='upper left',\n                        label='Train_Acc',col='red',\n                        img_title=img_title,suf=suf)\n        elif opt == 'Loss':\n            img_portray(opt=opt,write=write,dates=self.trainll,\n                        #location='upper right',\n                        linestyle=\"--\",\n                        label='Train_Loss',col='green',\n                        img_title=img_title,suf=suf)\n        if write:\n            save_images(img_title=img_title,suf=suf,opt=opt)\n        plt.show()\n        \n    def test(self,opt='Acc',write=True,custom_path=None,img_title=None,suf=None,**kwargs):\n        if opt == 'Acc':\n            img_portray(opt=opt,write=write,dates=self.testacl,\n                        #location='upper left',\n                        label='Test_Acc',col='red',\n                        xlabel=\"Batch_Size\",img_title=img_title,suf=suf)\n        elif opt == 'Loss':\n            img_portray(opt=opt,write=write,dates=self.testll,\n                        #location='upper right',\n                        linestyle=\"-.\",\n                        label='Test_Loss',col='green',\n                        img_title=img_title,suf=suf)\n        if write:\n            save_images(split='test',img_title=img_title,suf=suf,opt=opt)\n        plt.show()\n        \n    def val(self,opt='Acc',write=True,custom_path=None,img_title=None,suf=None):\n        if opt == 'Acc':\n            img_portray(opt=opt,write=write,dates=self.valacl,\n                        linestyle=\"dotted\",col='red',\n                        label='Val_Acc',#location='upper left',\n                        img_title=img_title,suf=suf)\n        elif opt == 'Loss':\n            img_portray(opt=opt,write=write,dates=self.valll,\n                        linestyle=\"-.\",col='green',\n                        label='Val_Loss',#location='upper right',\n                        img_title=img_title,suf=suf)\n        if write:\n            save_images(split='Val',img_title=img_title,suf=suf,opt=opt)\n        plt.show()\n        \n    def conclusion(self,opt=\"test\",img_title=None):\n        if opt == \"test\" and len(self.testacl) != 0:\n            print(f'\\033[31m=================Conclusion====================\\033[0m')\n            best_idx = self.testacl.index(max(self.testacl))\n            # val_idx = (best_idx+1)-1\n            best_epoch = (best_idx+1)\n            print(f\"Dataset:[\\033[1;31m{img_title}\\033[0m]\")\n            print(f\"Best_Epoch [\\033[1;31m{best_epoch}\\033[0m]\")\n            # print(\"[Train] loss {self.trainll[best_epoch-1]};\")\n            print(f\"[Test] \\033[31mACC:{round(float(self.testacl[best_idx]),2)}%\\033[0m.\")\n            # Loss:{self.testll[best_idx]}, AUC:{round(float(self.testauc[best_idx]),2)}%\n            # print(f\"[Test]:\\033[32mVal_ACC:{round(float(max(self.testauc)),2)}%\\033[0m.\")\n        if opt == \"val\" and len(self.valacl) != 0:\n            print(f'\\033[31m=================Conclusion====================\\033[0m')\n            best_idx = self.valacl.index(max(self.valacl))\n            best_epoch = (best_idx+1)\n            print(f\"Dataset:[\\033[1;31m{img_title}\\033[0m]\")\n            print(f\"Best_Epoch [\\033[1;31m{best_epoch}\\033[0m]\")\n            print(f\"[Val] \\033[31mACC:{round(float(self.valacl[best_idx]),2)}%\\033[0m.\")\n\n        if opt == \"auc\" and len(self.testauc) != 0:\n            print(f'\\033[31m=================Conclusion====================\\033[0m')\n            best_idx = self.testauc.index(max(self.testauc))\n            val_idx = (best_idx+1)-1\n            best_epoch = (best_idx+1)\n            print(f\"Dataset:[\\033[1;31m{img_title}\\033[0m]\")\n            print(f\"Best_Epoch [\\033[1;31m{best_epoch}\\033[0m]\\n[Train] loss:{self.trainll[best_epoch-1]};\")\n            print(f\"[Test] Loss:{self.testll[best_idx]}, \\033[32mACC:{round(float(self.testacl[best_idx]),2)}%\\033[0m.\")\n            print(f\"[Test]:\\033[32m AUC:{round(float(self.testauc[best_idx]),2)}%\\033[0m.\")\n            \n\ndef draw_size_acc(data_dict,custom_path='./tmp',write=True,fn='Batch_Size',img_title=None,suf=None):\n    sx=[]\n    sy=[]\n\n    for i in range(len(data_dict)):\n        x=sorted(data_dict.items(), key=lambda x: x[0])[i][0]\n        y=sorted(data_dict.items(), key=lambda x: x[0])[i][1]\n        sx.append(x)\n        sy.append(y)\n\n    y_max = np.argmax(sy)\n    img_max = y_max + offset\n    show_max = round(float(sy[y_max]),3)\n    plt.plot(img_max,show_max ,'8')\n    plt.annotate(show_max,xy=(img_max,show_max),xytext=(img_max,show_max))\n    \n    plt.style.use(\"seaborn-paper\")\n    plt.plot(sx, sy,label=\"Test_Data\")\n    plt.ylabel(\"Accuracy\")\n    plt.xlabel(fn)\n    plt.legend(loc=\"best\") \n    if write:\n        plt.savefig(f'{custom_path}/{img_title}/{suf}/{fn}.png',dpi=300)\n    plt.show()\n    \n    \ndef one_hot(x, length):\n    batch_size = x.size(0)\n    x_one_hot = torch.zeros(batch_size, length)\n    for i in range(batch_size):\n        x_one_hot[i, x[i]] = 1.0\n    return x_one_hot\n\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n    \ndef confusion_matrix(evl_result,n_cla,cla_dict,data,img_title=None,suf=None):\n    plt.figure(figsize=(12,9))\n    sb = range(n_cla)\n\n    sns.heatmap(evl_result,cmap=\"Blues\",\n                annot=True,cbar=True,\n                fmt=\"g\", annot_kws={\"size\": 20})\n    plt.yticks([index + 0.5 for index in sb],cla_dict.values(),fontsize=16)\n    plt.xticks([index + 0.5 for index in sb],cla_dict.values(),fontsize=16)\n\n    plt.title(\"Confusion Matrix\",fontsize=24)\n    cax = plt.gcf().axes[-1]\n    cax.tick_params(labelsize=12)\n\n    # vname = lambda v,nms: [ vn for vn in nms if id(v)==id(nms[vn])][0]\n    # kn = vname(evl_result,locals())\n    if evl_result.sum().item() == len(data):\n        kn = 'test'\n    else:\n        kn = 'val'\n\n    plt.savefig(f\"./tmp/{img_title}/{suf}/Confusion_Matrix_{kn}.png\",dpi=300)\n    \n    \ndef pff(m_name,model,inputes):\n\n    print(\"%s | %s | %s | %s\" % (\"  Model  \", \"Params(M)\", \"FLOPs(G)\",\"FPS\"))\n    print(\"----------|-----------|----------|-----\")\n\n    total_ops, total_params = profile(model, (inputes,), verbose=False)\n    model.eval()\n    with torch.no_grad():\n        torch.cuda.synchronize()\n        start = time.time()\n        output= model(inputes)\n        torch.cuda.synchronize()\n        end = time.time()\n        single_fps = 1/(end-start)\n\n    print(\n        \"%s |    %.2f   |   %.2f   | %.1f\" % (m_name, total_params / (1000 ** 2),\n                                    total_ops / (1000 ** 3),\n                                    single_fps)\n        )\n    \n    \ndef metrics_scores(evl_result,n_classes,cla_dict):\n    P,R,F = 0,0,0\n    result_table = prettytable.PrettyTable()\n    result_table.field_names = ['Type','Precision', 'Recall', 'F1','Accuracy']    \n    accuracy = float(torch.sum(evl_result.diagonal())/torch.sum(evl_result))  \n\n    for i in range(n_classes):\n        pre = float(evl_result[i][i] / (torch.sum(evl_result,0)[i]) + 1e-12)\n        P += pre\n        recall = float(evl_result[i][i] / (torch.sum(evl_result,1)[i]) + 1e-12)\n        R += recall\n        F1 = pre * recall * 2 / (pre + recall + 1e-12)\n        F += F1\n        result_table.add_row([cla_dict[i], round(pre, 4), round(recall, 4), round(F1, 4),\" \"])\n    P_avg,R_avg,F_avg = P/n_classes,R/n_classes,F/n_classes\n    result_table.add_row([\"Total:\",round(P_avg, 4),round(R_avg, 4),round(F_avg, 4),round(accuracy,4)])\n    print(result_table)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:20.169932Z","iopub.execute_input":"2024-05-01T21:05:20.170767Z","iopub.status.idle":"2024-05-01T21:05:21.532967Z","shell.execute_reply.started":"2024-05-01T21:05:20.170719Z","shell.execute_reply":"2024-05-01T21:05:21.532033Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install torch-summary\nimport torch\nimport sys, os\nimport json\nimport torch.nn as nn  \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# from torch.utils.tensorboard import SummaryWriter\nimport prettytable\nimport time, random,timeit\nsys.setrecursionlimit(15000)\nfrom thop.profile import profile\n\nfrom PIL import Image\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchsummary import summary\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\n\nsetup_seed(3047)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:21.534288Z","iopub.execute_input":"2024-05-01T21:05:21.534818Z","iopub.status.idle":"2024-05-01T21:05:37.119587Z","shell.execute_reply.started":"2024-05-01T21:05:21.534787Z","shell.execute_reply":"2024-05-01T21:05:37.118296Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\nDownloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n","output_type":"stream"}]},{"cell_type":"code","source":"sys.path.append(os.pardir)\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\nimg_title = \"HAM10000\"#\"Skin_Cancer\"\nbest_acc = 0.\neval_acc = 0.\nbest_train = 0.\ndict_batch = {}\ndict_imgSize = {}\n\n\ntry:\n    print(len(train_acc_list))\nexcept NameError:\n    train_loss_list = []\n    train_acc_list = []\n    test_loss_list = []\n    test_acc_list = []\n    test_auc_list = []\n    val_loss_list = []\n    val_acc_list = []\n#activate ImageShow\nshow = ImageShow(train_loss_list = train_loss_list,\n                 train_acc_list = train_acc_list,\n                test_loss_list = test_loss_list,\n                test_acc_list = test_acc_list,\n                test_auc_list = test_auc_list,\n                val_loss_list = val_loss_list,\n                val_acc_list = val_acc_list,\n                )","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:37.122280Z","iopub.execute_input":"2024-05-01T21:05:37.122673Z","iopub.status.idle":"2024-05-01T21:05:37.192227Z","shell.execute_reply.started":"2024-05-01T21:05:37.122641Z","shell.execute_reply":"2024-05-01T21:05:37.190968Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n#                                  std=[0.229, 0.224, 0.225])\nnormalize = transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n# Resize = transforms.Resize((299,299))\n\ndef get_data(mode='ALL'):\n    global test_dataset,train_loader,val_loader,test_loader\n    global train_num,val_num,test_num,n_classes,cla_dict\n    # vt = int(trans)\n    data_transform = {\n        \"train\": transforms.Compose([#Resize,\n                                     transforms.RandomVerticalFlip(),\n                                     transforms.ToTensor(),\n                                     normalize]),\n        \"val\": transforms.Compose([#Resize,\n                                   transforms.ToTensor(),\n                                   normalize]),\n        \"test\": transforms.Compose([#Resize,\n                                    transforms.ToTensor(),\n                                    normalize]),}    \n    if mode == 'ALL':\n        train_dataset = datasets.ImageFolder(root=train_dir,transform=data_transform[\"train\"])\n        val_dataset = datasets.ImageFolder(root=val_dir,transform=data_transform[\"val\"])\n        test_dataset = datasets.ImageFolder(root=test_dir,transform=data_transform[\"test\"])\n\n        train_num = len(train_dataset)\n        val_num = len(val_dataset)\n        test_num = len(test_dataset)\n\n        train_loader = DataLoader(train_dataset,batch_size=BatchSize,\n                                                   pin_memory=pin_memory,\n                                                   shuffle=True,num_workers=nw)\n        val_loader = DataLoader(val_dataset,batch_size=V_size,\n                                                   pin_memory=pin_memory,\n                                                   shuffle=False,num_workers=nw)\n        test_loader = DataLoader(test_dataset,batch_size=T_size,\n                                                  pin_memory=pin_memory,\n                                                  shuffle=False,num_workers=nw)\n\n        print(\"using {} images for training, {} images for validation, {} images for testing.\".format(train_num,\n                                                                                                      val_num,\n                                                                                                      test_num))\n    else:\n        test_dataset = datasets.ImageFolder(root=test_dir,transform=data_transform[\"test\"])\n        test_num = len(test_dataset)\n        test_loader = DataLoader(test_dataset,batch_size=T_size,\n                                                  pin_memory=pin_memory,\n                                                  shuffle=False,num_workers=nw)\n        print(f\"using {test_num} images for testing.\")\n    \n    data_list = test_dataset.class_to_idx\n    cla_dict = dict((val, key) for key, val in data_list.items())\n    n_classes  = len(data_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:37.193832Z","iopub.execute_input":"2024-05-01T21:05:37.194200Z","iopub.status.idle":"2024-05-01T21:05:37.264196Z","shell.execute_reply.started":"2024-05-01T21:05:37.194170Z","shell.execute_reply":"2024-05-01T21:05:37.262979Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nBatchSize = 168\nV_size = 21\nT_size = 21\n\n\ntrain_dir='/kaggle/input/ham10000-augmented/ham10000_augmented/train525e384'\nval_dir='/kaggle/input/ham10000-augmented/ham10000_augmented/val525e384png'\ntest_dir='/kaggle/input/ham10000-augmented/ham10000_augmented/test525png384'\npin_memory = True\nnw = 6#min([os.cpu_count(), BatchSize if BatchSize > 1 else 0, 6]) \nprint(f'Using {nw} dataloader workers every process.')\nget_data()\nprint(f'Using {n_classes} classes.')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:05:37.265615Z","iopub.execute_input":"2024-05-01T21:05:37.266059Z","iopub.status.idle":"2024-05-01T21:06:53.750438Z","shell.execute_reply.started":"2024-05-01T21:05:37.266025Z","shell.execute_reply":"2024-05-01T21:06:53.749306Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Using 6 dataloader workers every process.\nusing 51646 images for training, 1006 images for validation, 828 images for testing.\nUsing 7 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"n_channels = 3 #RGB\n\nnetwork = CapsNet(conv_inputs=n_channels, \n                     num_classes=n_classes,# category_number\n                     init_weights=True,)\nnetwork = network.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:27:23.814291Z","iopub.execute_input":"2024-05-01T21:27:23.815431Z","iopub.status.idle":"2024-05-01T21:27:23.925843Z","shell.execute_reply.started":"2024-05-01T21:27:23.815389Z","shell.execute_reply":"2024-05-01T21:27:23.924640Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.123\noptimizer = optim.Adam(network.parameters(), lr=learning_rate)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min=1e-8, last_epoch=-1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:27:24.035360Z","iopub.execute_input":"2024-05-01T21:27:24.035780Z","iopub.status.idle":"2024-05-01T21:27:24.095770Z","shell.execute_reply.started":"2024-05-01T21:27:24.035748Z","shell.execute_reply":"2024-05-01T21:27:24.094646Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    network.train()\n    global best_train,train_evl_result#,evl_tmp_result\n    running_loss,r_pre = 0., 0.\n    print_step = len(train_loader)//2\n    steps_num = len(train_loader)\n    tmp_size = BatchSize\n    print(f'\\033[1;32m[Train Epoch:[{epoch}]{img_title} ==> Training]\\033[0m ...')\n    optimizer.zero_grad()\n    train_tmp_result = torch.zeros(n_classes,n_classes)\n    startT = timeit.default_timer() \n    \n    for batch_idx, (data, target) in enumerate(train_loader):        \n\n        batch_idx += 1\n        target_indices = target\n        target_one_hot = one_hot(target, length=n_classes)\n        data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n\n        output = network(data)\n        loss = network.loss(output, target, size_average=True)       \n        loss.backward()     \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        running_loss += loss.item()\n        \n        # v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True)) \n        # v_mag = torch.norm(output,p=2,dim=(2,3), keepdim=True)\n        v_mag = LA.norm(output,ord='nuc',dim=(2,3), keepdim=True)#‘fro’ (default)\n        pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n        r_pre += pred.eq(target_indices.view_as(pred)).squeeze().sum()\n        tmp_pre = r_pre/(batch_idx*BatchSize)\n        \n        if batch_idx % print_step == 0 and batch_idx != steps_num:\n            print(\"[{}/{}] Loss{:.5f},ACC:{:.5f}\".format(batch_idx,len(train_loader),\n                                                         loss,tmp_pre))\n        if batch_idx % steps_num == 0 and train_num % tmp_size != 0:\n            tmp_size = train_num % tmp_size\n                          \n        for i in range(tmp_size):\n            pred_x = pred.numpy()\n            train_tmp_result[target_indices[i]][pred_x[i]] +=1\n\n        #if best_train < tmp_pre and tmp_pre >= 90: \n        #    torch.save(network.state_dict(), iter_path)\n        \n    epoch_acc = r_pre / train_num\n    epoch_loss = running_loss / len(train_loader)  \n    train_loss_list.append(epoch_loss)\n    train_acc_list.append(epoch_acc) \n    scheduler.step()\n    if best_train < epoch_acc:\n        best_train = epoch_acc\n        train_evl_result = train_tmp_result.clone()\n        #torch.save(network.state_dict(), last_path)\n        #torch.save(train_evl_result, f'./tmp/{img_title}/{dirs}/train_evl_result.pth')\n    \n    endT = timeit.default_timer()\n    run_time = endT-startT\n    print(\"Train Epoch:[{}] Running:[{:.2f}s], Loss:{:.5f},Acc:{:.5f},Best_train:{:.5f}\".format(epoch,run_time,\n                                                                                                 epoch_loss,\n                                                                                                 epoch_acc,best_train))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:26:01.704657Z","iopub.execute_input":"2024-05-01T21:26:01.705469Z","iopub.status.idle":"2024-05-01T21:26:01.777962Z","shell.execute_reply.started":"2024-05-01T21:26:01.705427Z","shell.execute_reply":"2024-05-01T21:26:01.776707Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def test(split=\"test\"):\n    network.eval()\n    global test_acc,eval_acc,best_acc,net_parameters\n    global test_evl_result,val_evl_result#,evl_tmp_result\n    cor_loss,correct,Auc, Acc= 0, 0, 0, 0\n    evl_tmp_result = torch.zeros(n_classes,n_classes)\n    \n    if split == 'val':\n        data_loader = val_loader\n        tmp_size = V_size\n        data_num = val_num\n    else:\n        data_loader = test_loader\n        tmp_size = T_size\n        data_num = test_num\n        \n    steps_num = len(data_loader)\n    print(f'\\033[35m{img_title} ==> {split} ...\\033[0m')\n    \n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            batch_idx +=1\n            target_indices = target#torch.Size([batch, 7])  \n            target_one_hot = one_hot(target, length=n_classes)            \n            data, target = Variable(data).to(device), Variable(target_one_hot).to(device)\n\n            output= network(data)#torch.Size([batch_size, 7, 16, 1])         \n            v_mag = LA.norm(output,ord='nuc',dim=(2,3), keepdim=True)#\n            pred = v_mag.data.max(1, keepdim=True)[1].cpu()#[9, 2, 1, 1, 6,..., 1, 4, 6, 5, 7,]\n            \n            if batch_idx % steps_num == 0 and data_num % tmp_size != 0:\n                tmp_size = data_num % tmp_size\n                          \n            for i in range(tmp_size):\n                pred_y = pred.numpy()\n                evl_tmp_result[target_indices[i]][pred_y[i]] +=1 \n\n        diag_sum = torch.sum(evl_tmp_result.diagonal())\n        all_sum = torch.sum(evl_tmp_result) \n        test_acc = 100. * float(torch.div(diag_sum,all_sum)) \n        print(f\"{split}_Acc:\\033[1;32m{round(float(test_acc),3)}%\\033[0m\")\n\n        if split == 'val':\n            val_acc_list.append(test_acc)\n            if test_acc >= best_acc:\n                best_acc = test_acc\n                val_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n            print(f\"Best_val:\\033[1;32m[{round(float(best_acc),3)}%]\\033[0m\")\n        else:\n            test_acc_list.append(test_acc)\n            if test_acc >= eval_acc:\n                eval_acc = test_acc\n                test_evl_result = evl_tmp_result.clone()#copy.deepcopy(input)\n            print(f\"Best_eval:\\033[1;32m[{round(float(eval_acc),3)}%]\\033[0m\")  ","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:26:08.598984Z","iopub.execute_input":"2024-05-01T21:26:08.599421Z","iopub.status.idle":"2024-05-01T21:26:08.669676Z","shell.execute_reply.started":"2024-05-01T21:26:08.599387Z","shell.execute_reply":"2024-05-01T21:26:08.668200Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_epochs= 128","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:07:09.143286Z","iopub.execute_input":"2024-05-01T21:07:09.143702Z","iopub.status.idle":"2024-05-01T21:07:09.201957Z","shell.execute_reply.started":"2024-05-01T21:07:09.143671Z","shell.execute_reply":"2024-05-01T21:07:09.200756Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, num_epochs + 1): \n    train(epoch)\n    test('val')\n    \nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:27:28.574598Z","iopub.execute_input":"2024-05-01T21:27:28.575596Z","iopub.status.idle":"2024-05-02T07:22:44.557489Z","shell.execute_reply.started":"2024-05-01T21:27:28.575560Z","shell.execute_reply":"2024-05-02T07:22:44.555563Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[1;32m[Train Epoch:[1]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.30685,ACC:0.30492\nTrain Epoch:[1] Running:[457.36s], Loss:0.35631,Acc:0.41637,Best_train:0.41637\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m74.652%\u001b[0m\nBest_val:\u001b[1;32m[74.652%]\u001b[0m\n\u001b[1;32m[Train Epoch:[2]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.26527,ACC:0.59427\nTrain Epoch:[2] Running:[454.32s], Loss:0.26089,Acc:0.60857,Best_train:0.60857\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m68.787%\u001b[0m\nBest_val:\u001b[1;32m[74.652%]\u001b[0m\n\u001b[1;32m[Train Epoch:[3]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.28835,ACC:0.66068\nTrain Epoch:[3] Running:[454.41s], Loss:0.22544,Acc:0.66789,Best_train:0.66789\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m81.71%\u001b[0m\nBest_val:\u001b[1;32m[81.71%]\u001b[0m\n\u001b[1;32m[Train Epoch:[4]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.20080,ACC:0.70485\nTrain Epoch:[4] Running:[455.09s], Loss:0.20069,Acc:0.70941,Best_train:0.70941\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m84.791%\u001b[0m\nBest_val:\u001b[1;32m[84.791%]\u001b[0m\n\u001b[1;32m[Train Epoch:[5]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.15951,ACC:0.72940\nTrain Epoch:[5] Running:[455.78s], Loss:0.18511,Acc:0.73334,Best_train:0.73334\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m85.487%\u001b[0m\nBest_val:\u001b[1;32m[85.487%]\u001b[0m\n\u001b[1;32m[Train Epoch:[6]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.18582,ACC:0.74420\nTrain Epoch:[6] Running:[455.47s], Loss:0.18131,Acc:0.74219,Best_train:0.74219\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m85.388%\u001b[0m\nBest_val:\u001b[1;32m[85.487%]\u001b[0m\n\u001b[1;32m[Train Epoch:[7]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.16935,ACC:0.74030\nTrain Epoch:[7] Running:[454.55s], Loss:0.18067,Acc:0.74002,Best_train:0.74219\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.674%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[8]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.20610,ACC:0.73755\nTrain Epoch:[8] Running:[453.43s], Loss:0.18326,Acc:0.73735,Best_train:0.74219\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.382%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[9]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.17275,ACC:0.73006\nTrain Epoch:[9] Running:[452.47s], Loss:0.18319,Acc:0.73605,Best_train:0.74219\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.68%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[10]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.16294,ACC:0.73736\nTrain Epoch:[10] Running:[452.88s], Loss:0.17886,Acc:0.74041,Best_train:0.74219\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m53.181%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[11]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.15277,ACC:0.74590\nTrain Epoch:[11] Running:[452.76s], Loss:0.17194,Acc:0.75369,Best_train:0.75369\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m83.499%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[12]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.18861,ACC:0.77049\nTrain Epoch:[12] Running:[452.70s], Loss:0.15949,Acc:0.77297,Best_train:0.77297\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m84.891%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[13]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.12835,ACC:0.79186\nTrain Epoch:[13] Running:[451.88s], Loss:0.14756,Acc:0.79017,Best_train:0.79017\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.183%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[14]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.13876,ACC:0.80867\nTrain Epoch:[14] Running:[451.62s], Loss:0.13491,Acc:0.81187,Best_train:0.81187\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.376%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[15]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.12413,ACC:0.82526\nTrain Epoch:[15] Running:[452.13s], Loss:0.12635,Acc:0.82581,Best_train:0.82581\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.276%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[16]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.15795,ACC:0.83442\nTrain Epoch:[16] Running:[451.76s], Loss:0.12477,Acc:0.83116,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.481%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[17]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.15508,ACC:0.82703\nTrain Epoch:[17] Running:[451.86s], Loss:0.12467,Acc:0.82901,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m85.089%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[18]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.10937,ACC:0.81849\nTrain Epoch:[18] Running:[451.57s], Loss:0.12932,Acc:0.82177,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.282%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[19]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.14134,ACC:0.81231\nTrain Epoch:[19] Running:[452.42s], Loss:0.13551,Acc:0.81226,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m82.903%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[20]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.12994,ACC:0.80516\nTrain Epoch:[20] Running:[452.73s], Loss:0.13970,Acc:0.80481,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.084%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[21]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.13321,ACC:0.80956\nTrain Epoch:[21] Running:[452.15s], Loss:0.13601,Acc:0.81044,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m83.996%\u001b[0m\nBest_val:\u001b[1;32m[87.674%]\u001b[0m\n\u001b[1;32m[Train Epoch:[22]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.14943,ACC:0.82545\nTrain Epoch:[22] Running:[451.11s], Loss:0.12850,Acc:0.82370,Best_train:0.83116\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.773%\u001b[0m\nBest_val:\u001b[1;32m[87.773%]\u001b[0m\n\u001b[1;32m[Train Epoch:[23]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.13921,ACC:0.83631\nTrain Epoch:[23] Running:[451.47s], Loss:0.12024,Acc:0.83615,Best_train:0.83615\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m89.264%\u001b[0m\nBest_val:\u001b[1;32m[89.264%]\u001b[0m\n\u001b[1;32m[Train Epoch:[24]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08694,ACC:0.85030\nTrain Epoch:[24] Running:[450.92s], Loss:0.10972,Acc:0.85294,Best_train:0.85294\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.954%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[25]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.10579,ACC:0.86279\nTrain Epoch:[25] Running:[451.67s], Loss:0.10309,Acc:0.86289,Best_train:0.86289\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.954%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[26]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11447,ACC:0.86673\nTrain Epoch:[26] Running:[451.50s], Loss:0.10126,Acc:0.86651,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.855%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[27]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11587,ACC:0.86545\nTrain Epoch:[27] Running:[451.87s], Loss:0.10164,Acc:0.86584,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.855%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[28]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09784,ACC:0.85880\nTrain Epoch:[28] Running:[451.03s], Loss:0.10718,Acc:0.85676,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.457%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[29]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.10733,ACC:0.85220\nTrain Epoch:[29] Running:[451.24s], Loss:0.11401,Acc:0.84872,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.376%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[30]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.10976,ACC:0.84052\nTrain Epoch:[30] Running:[451.93s], Loss:0.11792,Acc:0.84099,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.966%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[31]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11438,ACC:0.83890\nTrain Epoch:[31] Running:[450.89s], Loss:0.11894,Acc:0.83927,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.072%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[32]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.10881,ACC:0.85378\nTrain Epoch:[32] Running:[451.02s], Loss:0.11145,Acc:0.85215,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m83.4%\u001b[0m\nBest_val:\u001b[1;32m[90.954%]\u001b[0m\n\u001b[1;32m[Train Epoch:[33]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11444,ACC:0.86244\nTrain Epoch:[33] Running:[451.94s], Loss:0.10315,Acc:0.86526,Best_train:0.86651\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.545%\u001b[0m\nBest_val:\u001b[1;32m[92.545%]\u001b[0m\n\u001b[1;32m[Train Epoch:[34]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08983,ACC:0.87457\nTrain Epoch:[34] Running:[451.03s], Loss:0.09511,Acc:0.87621,Best_train:0.87621\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.867%\u001b[0m\nBest_val:\u001b[1;32m[92.545%]\u001b[0m\n\u001b[1;32m[Train Epoch:[35]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08388,ACC:0.88710\nTrain Epoch:[35] Running:[452.03s], Loss:0.08895,Acc:0.88586,Best_train:0.88586\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.346%\u001b[0m\nBest_val:\u001b[1;32m[92.545%]\u001b[0m\n\u001b[1;32m[Train Epoch:[36]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.07322,ACC:0.88617\nTrain Epoch:[36] Running:[451.50s], Loss:0.08697,Acc:0.88770,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.048%\u001b[0m\nBest_val:\u001b[1;32m[92.545%]\u001b[0m\n\u001b[1;32m[Train Epoch:[37]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08122,ACC:0.88841\nTrain Epoch:[37] Running:[451.18s], Loss:0.08794,Acc:0.88725,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.942%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[38]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09612,ACC:0.87763\nTrain Epoch:[38] Running:[451.80s], Loss:0.09370,Acc:0.87949,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.855%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[39]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11536,ACC:0.87218\nTrain Epoch:[39] Running:[451.26s], Loss:0.09957,Acc:0.86965,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.557%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[40]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08354,ACC:0.86584\nTrain Epoch:[40] Running:[451.74s], Loss:0.10504,Acc:0.86473,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.966%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[41]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08955,ACC:0.85927\nTrain Epoch:[41] Running:[451.59s], Loss:0.10548,Acc:0.86204,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.767%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[42]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08297,ACC:0.86959\nTrain Epoch:[42] Running:[452.23s], Loss:0.10085,Acc:0.86882,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.247%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[43]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11198,ACC:0.88200\nTrain Epoch:[43] Running:[451.49s], Loss:0.09286,Acc:0.88346,Best_train:0.88770\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m91.153%\u001b[0m\nBest_val:\u001b[1;32m[92.942%]\u001b[0m\n\u001b[1;32m[Train Epoch:[44]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09296,ACC:0.89135\nTrain Epoch:[44] Running:[452.41s], Loss:0.08471,Acc:0.89368,Best_train:0.89368\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.141%\u001b[0m\nBest_val:\u001b[1;32m[93.141%]\u001b[0m\n\u001b[1;32m[Train Epoch:[45]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08988,ACC:0.90190\nTrain Epoch:[45] Running:[451.60s], Loss:0.07921,Acc:0.90321,Best_train:0.90321\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.936%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[46]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.07188,ACC:0.90550\nTrain Epoch:[46] Running:[452.67s], Loss:0.07738,Acc:0.90553,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.638%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[47]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08996,ACC:0.90526\nTrain Epoch:[47] Running:[452.06s], Loss:0.07807,Acc:0.90367,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.539%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[48]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08981,ACC:0.89877\nTrain Epoch:[48] Running:[453.08s], Loss:0.08323,Acc:0.89763,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.042%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[49]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08473,ACC:0.88822\nTrain Epoch:[49] Running:[452.86s], Loss:0.08987,Acc:0.88826,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.258%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[50]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.14074,ACC:0.88412\nTrain Epoch:[50] Running:[451.91s], Loss:0.09407,Acc:0.88266,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.557%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[51]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.11361,ACC:0.88261\nTrain Epoch:[51] Running:[452.02s], Loss:0.09606,Acc:0.87862,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m88.867%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[52]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09217,ACC:0.88613\nTrain Epoch:[52] Running:[452.05s], Loss:0.09064,Acc:0.88725,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m91.75%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[53]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09630,ACC:0.89467\nTrain Epoch:[53] Running:[451.58s], Loss:0.08406,Acc:0.89734,Best_train:0.90553\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.048%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[54]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08156,ACC:0.90816\nTrain Epoch:[54] Running:[452.37s], Loss:0.07655,Acc:0.90729,Best_train:0.90729\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.141%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[55]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.06915,ACC:0.91690\nTrain Epoch:[55] Running:[452.14s], Loss:0.07156,Acc:0.91680,Best_train:0.91680\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.247%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[56]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08528,ACC:0.92053\nTrain Epoch:[56] Running:[452.01s], Loss:0.06972,Acc:0.91951,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.539%\u001b[0m\nBest_val:\u001b[1;32m[93.936%]\u001b[0m\n\u001b[1;32m[Train Epoch:[57]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05400,ACC:0.91802\nTrain Epoch:[57] Running:[452.92s], Loss:0.07053,Acc:0.91810,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m94.632%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[58]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09798,ACC:0.90967\nTrain Epoch:[58] Running:[452.47s], Loss:0.07626,Acc:0.90965,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.545%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[59]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.07546,ACC:0.89989\nTrain Epoch:[59] Running:[452.02s], Loss:0.08362,Acc:0.89951,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m89.066%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[60]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08976,ACC:0.89120\nTrain Epoch:[60] Running:[452.21s], Loss:0.08762,Acc:0.89231,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m85.388%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[61]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09557,ACC:0.89065\nTrain Epoch:[61] Running:[452.47s], Loss:0.08952,Acc:0.89008,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m86.183%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[62]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09917,ACC:0.90024\nTrain Epoch:[62] Running:[452.74s], Loss:0.08378,Acc:0.89972,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.06%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[63]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08622,ACC:0.90805\nTrain Epoch:[63] Running:[451.71s], Loss:0.07783,Acc:0.90803,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.942%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[64]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.07579,ACC:0.91794\nTrain Epoch:[64] Running:[451.90s], Loss:0.07100,Acc:0.91781,Best_train:0.91951\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m91.551%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[65]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.07118,ACC:0.92733\nTrain Epoch:[65] Running:[452.22s], Loss:0.06496,Acc:0.92749,Best_train:0.92749\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m94.036%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[66]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05713,ACC:0.92973\nTrain Epoch:[66] Running:[451.56s], Loss:0.06373,Acc:0.92987,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.837%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[67]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05422,ACC:0.93081\nTrain Epoch:[67] Running:[451.07s], Loss:0.06463,Acc:0.92890,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.638%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[68]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08226,ACC:0.92123\nTrain Epoch:[68] Running:[451.76s], Loss:0.06992,Acc:0.92001,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.241%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[69]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.06122,ACC:0.91551\nTrain Epoch:[69] Running:[451.02s], Loss:0.07660,Acc:0.91167,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m91.252%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[70]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.09447,ACC:0.90275\nTrain Epoch:[70] Running:[452.14s], Loss:0.08240,Acc:0.90315,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m91.849%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[71]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.06624,ACC:0.90530\nTrain Epoch:[71] Running:[451.40s], Loss:0.08138,Acc:0.90514,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m87.972%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[72]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.06728,ACC:0.90998\nTrain Epoch:[72] Running:[451.81s], Loss:0.07783,Acc:0.90865,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m83.996%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[73]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.08072,ACC:0.91895\nTrain Epoch:[73] Running:[452.23s], Loss:0.07181,Acc:0.91728,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m90.358%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[74]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.06899,ACC:0.92811\nTrain Epoch:[74] Running:[451.92s], Loss:0.06444,Acc:0.92966,Best_train:0.92987\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.042%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[75]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05144,ACC:0.93406\nTrain Epoch:[75] Running:[451.82s], Loss:0.05993,Acc:0.93564,Best_train:0.93564\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.141%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[76]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05784,ACC:0.93425\nTrain Epoch:[76] Running:[452.57s], Loss:0.05976,Acc:0.93500,Best_train:0.93564\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m92.744%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[77]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.04600,ACC:0.93835\nTrain Epoch:[77] Running:[451.95s], Loss:0.05908,Acc:0.93674,Best_train:0.93674\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m93.638%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[78]HAM10000 ==> Training]\u001b[0m ...\n[154/308] Loss0.05618,ACC:0.93116\nTrain Epoch:[78] Running:[451.81s], Loss:0.06385,Acc:0.93097,Best_train:0.93674\n\u001b[35mHAM10000 ==> val ...\u001b[0m\nval_Acc:\u001b[1;32m94.135%\u001b[0m\nBest_val:\u001b[1;32m[94.632%]\u001b[0m\n\u001b[1;32m[Train Epoch:[79]HAM10000 ==> Training]\u001b[0m ...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m): \n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m target_one_hot \u001b[38;5;241m=\u001b[39m one_hot(target, length\u001b[38;5;241m=\u001b[39mn_classes)\n\u001b[1;32m     18\u001b[0m data, target \u001b[38;5;241m=\u001b[39m Variable(data)\u001b[38;5;241m.\u001b[39mto(device), Variable(target_one_hot)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mloss(output, target, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)       \n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()     \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 64\u001b[0m, in \u001b[0;36mCapsNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPool(x)      \n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCBAM(x)\n\u001b[0;32m---> 64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 159\u001b[0m, in \u001b[0;36mDigits_Caps.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m u_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(W, x)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Initialize routing logits to zero.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m b_ij \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Convert routing logits to softmax.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# (batch, features, num_units, 1, 1)\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m#c_ij = F.softmax(b_ij, dim=0)\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"torch.save(network.state_dict(), 'gpu_78_epochs_capsnet.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:26:17.039671Z","iopub.execute_input":"2024-05-02T07:26:17.040092Z","iopub.status.idle":"2024-05-02T07:26:17.109821Z","shell.execute_reply.started":"2024-05-02T07:26:17.040059Z","shell.execute_reply":"2024-05-02T07:26:17.109071Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}